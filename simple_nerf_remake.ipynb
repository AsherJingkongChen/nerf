{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.types import Device\n",
    "\n",
    "\n",
    "class PointSampler(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        focal: float,\n",
    "        height: int,\n",
    "        width: int,\n",
    "        points_per_ray: int,\n",
    "        device: Device,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        `[4, 4] => [height, width, points_per_ray, 3 + 3 + 1]`\n",
    "        \"\"\"\n",
    "\n",
    "        super(PointSampler, self).__init__()\n",
    "\n",
    "        import torch\n",
    "\n",
    "        focal = float(focal)\n",
    "        height = int(height)\n",
    "        width = int(width)\n",
    "        points_per_ray = max(int(points_per_ray), 1)\n",
    "\n",
    "        self.directions = torch.stack(\n",
    "            torch.meshgrid(\n",
    "                (torch.arange(float(width), device=device) - width / 2.0) / focal,\n",
    "                (-torch.arange(float(height), device=device) + height / 2.0) / focal,\n",
    "                torch.tensor(-1.0, device=device),\n",
    "                indexing=\"xy\",\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "        self.points_per_ray = points_per_ray\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        posture: Tensor,\n",
    "        distance_range: tuple[float, float],\n",
    "        is_random: bool,\n",
    "    ) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        posture = torch.as_tensor(posture)[:3]\n",
    "        device = posture.device\n",
    "        distance_range = tuple(map(float, distance_range))\n",
    "        is_random = bool(is_random)\n",
    "        distance_max = max(distance_range)\n",
    "        distance_min = min(distance_range)\n",
    "        interval = (distance_max - distance_min) / self.points_per_ray\n",
    "\n",
    "        directions = (self.directions * posture[:, :3]).sum(dim=-1).unsqueeze(-2)\n",
    "        origins = posture[:, -1].expand_as(directions)\n",
    "        distances = (\n",
    "            torch.linspace(\n",
    "                distance_min,\n",
    "                distance_max,\n",
    "                self.points_per_ray,\n",
    "                device=device,\n",
    "            )\n",
    "            .repeat(\n",
    "                (*origins.shape[:-2], 1),\n",
    "            )\n",
    "            .unsqueeze(-1)\n",
    "        )\n",
    "        if is_random:\n",
    "            distances += (\n",
    "                torch.rand(*origins.shape[:-2], self.points_per_ray, 1, device=device)\n",
    "                * interval\n",
    "            )\n",
    "        points = origins + directions * distances\n",
    "        directions = directions.expand_as(points)\n",
    "        return torch.cat([points, directions, distances], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.types import Device\n",
    "\n",
    "\n",
    "class PositionalEncoder(Module):\n",
    "    def __init__(self, encoding_factor: int, device: Device):\n",
    "        \"\"\"\n",
    "        `[..., input_dimension] => [..., input_dimension * (2 * encoding_factor + 1)]`\n",
    "        \"\"\"\n",
    "\n",
    "        import torch\n",
    "\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "\n",
    "        encoding_factor = max(int(encoding_factor), 0)\n",
    "\n",
    "        freq_lvls = torch.arange(encoding_factor, device=device)\n",
    "        self.freq = ((2**freq_lvls) * torch.pi).repeat_interleave(2).unsqueeze_(-1)\n",
    "        sine_offsets = torch.tensor([0.0, torch.pi / 2], device=device)\n",
    "        self.offsets = sine_offsets.repeat(encoding_factor).unsqueeze_(-1)\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        inputs = torch.as_tensor(inputs).unsqueeze(-2)\n",
    "\n",
    "        features = (self.freq * inputs + self.offsets).sin()\n",
    "        features = torch.cat([inputs, features], dim=-2)\n",
    "        features = features.reshape(*inputs.shape[:-2], -1)\n",
    "        return features\n",
    "\n",
    "    def get_last_dimension(self, input_dimension: int) -> int:\n",
    "        return int(input_dimension) * (self.freq.shape[0] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "class ColorPredictor(Module):\n",
    "    def __init__(self, device: Device):\n",
    "        \"\"\"\n",
    "        `[..., 3 + 3 + 1] => [..., 4]`\n",
    "        \"\"\"\n",
    "\n",
    "        super(ColorPredictor, self).__init__()\n",
    "\n",
    "        from torch.nn import Linear, ModuleList, ReLU, Sigmoid\n",
    "\n",
    "        ENCODING_FACTOR = 14\n",
    "        self.encode = PositionalEncoder(ENCODING_FACTOR, device=device)\n",
    "\n",
    "        I = 3 + 3 + 1\n",
    "        I = self.encode.get_last_dimension(I)\n",
    "\n",
    "        O = 4\n",
    "        H = 256\n",
    "\n",
    "        self.layers = ModuleList(\n",
    "            [\n",
    "                Linear(I, H, device=device),\n",
    "                ReLU(),\n",
    "                Linear(H, H, device=device),\n",
    "                ReLU(),\n",
    "                Linear(H, H, device=device),\n",
    "                ReLU(),\n",
    "                Linear(H + I, H, device=device),\n",
    "                ReLU(),\n",
    "                Linear(H, H, device=device),\n",
    "                ReLU(),\n",
    "                Linear(H, H, device=device),\n",
    "                ReLU(),\n",
    "                Linear(H, H, device=device),\n",
    "                ReLU(),\n",
    "                Linear(H + I, H, device=device),\n",
    "                ReLU(),\n",
    "                Linear(H, O, device=device),\n",
    "                Sigmoid(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.skip_indexs = {6, 14}\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        inputs = self.encode(inputs)\n",
    "        outputs = inputs\n",
    "\n",
    "        for index, layer in enumerate(self.layers):\n",
    "            if index in self.skip_indexs:\n",
    "                outputs = torch.cat([outputs, inputs], dim=-1)\n",
    "            outputs = layer(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.types import Device\n",
    "\n",
    "\n",
    "class VolumeRenderer(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        focal: float,\n",
    "        height: int,\n",
    "        width: int,\n",
    "        points_per_ray: int,\n",
    "        device: Device,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        `[4, 4] => [height, width, 3]`\n",
    "        \"\"\"\n",
    "\n",
    "        super(VolumeRenderer, self).__init__()\n",
    "\n",
    "        self.sample = PointSampler(\n",
    "            focal=focal,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            points_per_ray=points_per_ray,\n",
    "            device=device,\n",
    "        )\n",
    "        self.predict = ColorPredictor(device=device)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        posture: Tensor,\n",
    "        rays_per_batch: int,\n",
    "        distance_range: tuple[float, float],\n",
    "        is_random: bool,\n",
    "    ) -> Tensor:\n",
    "        import torch\n",
    "        from torch.utils.data import DataLoader\n",
    "\n",
    "        rays_per_batch = max(int(rays_per_batch), 1)\n",
    "\n",
    "        points_per_batch = rays_per_batch * self.sample.points_per_ray\n",
    "        points: Tensor = self.sample(posture, distance_range, is_random)\n",
    "        colors = torch.cat(\n",
    "            [\n",
    "                self.predict(batch)\n",
    "                for batch in DataLoader(\n",
    "                    dataset=points.reshape(-1, points.shape[-1]),\n",
    "                    batch_size=points_per_batch,\n",
    "                )\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        colors = colors.reshape(*points.shape[:-1], -1)\n",
    "        rgb = colors[..., :3]\n",
    "        alpha = colors[..., 3]\n",
    "        distances = points[..., -1]\n",
    "        intervals = torch.cat(\n",
    "            [\n",
    "                distances[..., 1:] - distances[..., :-1],\n",
    "                torch.tensor([1e9]).expand_as(distances[..., -1:]),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        alpha = 1.0 - torch.exp(-alpha * intervals)\n",
    "        transmittance = (alpha * torch.cumprod(1.0 - alpha + 1e-10, -1)).unsqueeze(-1)\n",
    "        rgb_planar = (rgb * transmittance).sum(dim=-2)\n",
    "\n",
    "        return rgb_planar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch import Tensor\n",
    "from torch.types import Device\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ViewSynthesisDataset:\n",
    "    count: int\n",
    "    focal: float\n",
    "    height: int\n",
    "    images: Tensor\n",
    "    postures: Tensor\n",
    "    width: int\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if self.images.shape[0] != self.postures.shape[0]:\n",
    "            raise ValueError(\"The number of images and postures must be the same\")\n",
    "\n",
    "    @staticmethod\n",
    "    def from_numpy(url: str) -> \"ViewSynthesisDataset\":\n",
    "        from httpx import get\n",
    "        from io import BytesIO\n",
    "        from numpy import load\n",
    "\n",
    "        import torch\n",
    "\n",
    "        try:\n",
    "            file = BytesIO(\n",
    "                get(url, follow_redirects=True, timeout=60).raise_for_status().content\n",
    "            )\n",
    "        except:\n",
    "            file = open(url, \"rb\")\n",
    "\n",
    "        with file as file_entered:\n",
    "            arrays = load(file_entered)\n",
    "            focal = float(arrays[\"focal\"])\n",
    "            images = torch.as_tensor(arrays[\"images\"])\n",
    "            postures = torch.as_tensor(arrays[\"poses\"])\n",
    "\n",
    "        return ViewSynthesisDataset(\n",
    "            count=images.shape[0],\n",
    "            focal=focal,\n",
    "            height=images.shape[1],\n",
    "            images=images,\n",
    "            postures=postures,\n",
    "            width=images.shape[2],\n",
    "        )\n",
    "\n",
    "    def get_image_and_posture(self, index: int | None = None) -> tuple[Tensor, Tensor]:\n",
    "        from random import randint\n",
    "\n",
    "        if index is not None:\n",
    "            index = int(index)\n",
    "        else:\n",
    "            index = randint(0, self.count - 1)\n",
    "\n",
    "        return self.images[index], self.postures[index]\n",
    "\n",
    "    def set_device(self, device: Device) -> \"ViewSynthesisDataset\":\n",
    "        self.images = self.images.to(device)\n",
    "        self.postures = self.postures.to(device)\n",
    "        return self\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        repr = f\"{self.__class__.__name__}(\"\n",
    "        for name, value in self.__dict__.items():\n",
    "            if isinstance(value, Tensor):\n",
    "                value = f\"Tensor(shape={tuple(value.shape)}, dtype={value.dtype})\"\n",
    "            elif type(value) is float:\n",
    "                value = f\"{value:.7f}\"\n",
    "            repr += f\"\\n  {name}={value},\"\n",
    "        repr += \"\\n)\"\n",
    "        return repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: ViewSynthesisDataset,\n",
    "        train_ratio: float,\n",
    "        device: Device,\n",
    "    ):\n",
    "        train_data_count = max(int(round(dataset.count * train_ratio)), 1)\n",
    "        test_data_count = dataset.count - train_data_count\n",
    "        data_split_index = -test_data_count\n",
    "        focal = dataset.focal\n",
    "        height = dataset.height\n",
    "        images = dataset.images\n",
    "        postures = dataset.postures\n",
    "        width = dataset.width\n",
    "\n",
    "        self.test_dataset = ViewSynthesisDataset(\n",
    "            count=test_data_count,\n",
    "            focal=focal,\n",
    "            height=height,\n",
    "            images=images[data_split_index:],\n",
    "            postures=postures[data_split_index:],\n",
    "            width=width,\n",
    "        ).set_device(device)\n",
    "\n",
    "        self.train_dataset = ViewSynthesisDataset(\n",
    "            count=train_data_count,\n",
    "            focal=focal,\n",
    "            height=height,\n",
    "            images=images[:data_split_index],\n",
    "            postures=postures[:data_split_index],\n",
    "            width=width,\n",
    "        ).set_device(device)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        render: VolumeRenderer,\n",
    "        epochs: int,\n",
    "        rays_per_batch: int,\n",
    "        distance_range: tuple[float, float],\n",
    "        show_progress: bool,\n",
    "    ) -> None:\n",
    "        import torch\n",
    "        from torch.nn import MSELoss\n",
    "        from torch.optim import Adam\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        EPOCHS_PER_DEMO = 20\n",
    "\n",
    "        criterion = MSELoss()\n",
    "        optimizer = Adam(render.parameters(), lr=5e-4)\n",
    "        progress = tqdm(\n",
    "            disable=not show_progress,\n",
    "            desc=f\"Fitting the renderer to {self.train_dataset.count}x images and postures\",\n",
    "            colour=\"green\",\n",
    "            dynamic_ncols=True,\n",
    "            total=epochs,\n",
    "        )\n",
    "\n",
    "        with progress:\n",
    "            for epoch in range(epochs):\n",
    "                true_image, posture = self.train_dataset.get_image_and_posture()\n",
    "                optimizer.zero_grad()\n",
    "                rendered_image: Tensor = render(\n",
    "                    posture=posture,\n",
    "                    rays_per_batch=rays_per_batch,\n",
    "                    distance_range=distance_range,\n",
    "                    is_random=True,\n",
    "                )\n",
    "                loss = criterion(rendered_image, true_image)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if show_progress and epoch % EPOCHS_PER_DEMO == 0:\n",
    "                    with torch.no_grad():\n",
    "                        true_image_demo, posture_demo = (\n",
    "                            self.train_dataset.get_image_and_posture(0)\n",
    "                        )\n",
    "                        rendered_image_demo = render(\n",
    "                            posture=posture_demo,\n",
    "                            rays_per_batch=rays_per_batch,\n",
    "                            distance_range=distance_range,\n",
    "                            is_random=False,\n",
    "                        )\n",
    "                        Trainer.display(\n",
    "                            torch.cat([true_image_demo, rendered_image_demo], dim=1)\n",
    "                        )\n",
    "\n",
    "                progress.update()\n",
    "\n",
    "    def test(\n",
    "        self,\n",
    "        render: VolumeRenderer,\n",
    "        rays_per_batch: int,\n",
    "        distance_range: tuple[float, float],\n",
    "    ) -> None:\n",
    "        import torch\n",
    "        from torch.nn.functional import mse_loss\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for index in range(self.test_dataset.count):\n",
    "                true_image, posture = self.test_dataset.get_image_and_posture(index)\n",
    "                rendered_image = render(\n",
    "                    posture=posture,\n",
    "                    rays_per_batch=rays_per_batch,\n",
    "                    distance_range=distance_range,\n",
    "                    is_random=False,\n",
    "                )\n",
    "                quality_mse = mse_loss(rendered_image, true_image).item()\n",
    "                quality_psnr = -10 * torch.log10(quality_mse)\n",
    "                display(\n",
    "                    dict(\n",
    "                        test_index=index,\n",
    "                        quality=dict(mse=quality_mse, psnr=quality_psnr),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                Trainer.display(torch.cat([true_image, rendered_image], dim=1))\n",
    "\n",
    "    @staticmethod\n",
    "    def display(image: Tensor):\n",
    "        from IPython.display import display\n",
    "        from PIL import Image\n",
    "        from torch import uint8\n",
    "\n",
    "        return display(\n",
    "            Image.fromarray((image * 255).round().type(uint8).numpy(force=True))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    from torch import cuda, save\n",
    "    \n",
    "    EPOCHS = 1000\n",
    "    RAYS_PER_BATCH = 1024\n",
    "    POINTS_PER_RAY = 32\n",
    "    DISTANCE_RANGE = (2.0, 6.0)\n",
    "\n",
    "    dataset = ViewSynthesisDataset.from_numpy(\n",
    "        \"https://raw.githubusercontent.com/AsherJingkongChen/nerf/main/tiny_nerf_data.npz\"\n",
    "    )\n",
    "    device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "    render = VolumeRenderer(\n",
    "        focal=dataset.focal,\n",
    "        height=dataset.height,\n",
    "        width=dataset.width,\n",
    "        points_per_ray=POINTS_PER_RAY,\n",
    "        device=device,\n",
    "    )\n",
    "    trainer = Trainer(dataset=dataset, train_ratio=0.8, device=device)\n",
    "\n",
    "    display(dict(dataset=dataset, device=device, render=render, trainer=trainer))\n",
    "\n",
    "    trainer.train(\n",
    "        render=render,\n",
    "        epochs=EPOCHS,\n",
    "        rays_per_batch=RAYS_PER_BATCH,\n",
    "        distance_range=DISTANCE_RANGE,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    trainer.test(\n",
    "        render=render,\n",
    "        rays_per_batch=RAYS_PER_BATCH,\n",
    "        distance_range=DISTANCE_RANGE,\n",
    "    )\n",
    "    save(render.state_dict(), \"VolumeRenderer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
