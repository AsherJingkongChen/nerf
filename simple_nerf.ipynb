{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @page {\n",
       "        size: A3 landscape;\n",
       "        margin: 0;\n",
       "    }\n",
       "    .jp-RenderedMermaid {\n",
       "        justify-content: center;\n",
       "    }\n",
       "    h2 {\n",
       "        page-break-before: always;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    @page {\n",
    "        size: A3 landscape;\n",
    "        margin: 0;\n",
    "    }\n",
    "    .jp-RenderedMermaid {\n",
    "        justify-content: center;\n",
    "    }\n",
    "    h2 {\n",
    "        page-break-before: always;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Synthesis\n",
    "\n",
    "> Implement by NeRF in pyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Description\n",
    "\n",
    "\"View synthesis\" is a task which\n",
    "generating images of a 3D scene from a specific point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Description\n",
    "\n",
    "\"NeRF\" (Neural Radiance Field) solved \"View synthesis\"\n",
    "by representing 3D scene using a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description\n",
    "\n",
    "1. Preprocessing\n",
    "2. Inference\n",
    "3. Rendering\n",
    "4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Preprocessing\n",
    "\n",
    "{{ True image }} → {{ Position, Direction, True color }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Inference\n",
    "\n",
    "{{ Position, Direction }} → {{ Volumetric sampling }} → {{ Positional encoding }} → {{ Network }} → {{ Color, Density }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Rendering\n",
    "\n",
    "{{ Color, Density }} → {{ Alpha blending }} → {{ Rendered color }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Training\n",
    "\n",
    "{{ True color, Rendered color, Network }} → {{ Network }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    subgraph Preprocessing\n",
    "        ti[True Image]\n",
    "        cp[Camera Posture]\n",
    "        tc[True Color]\n",
    "    end\n",
    "\n",
    "    subgraph Inference\n",
    "        vs[Volume Sampling]\n",
    "        B --> C[Ray Generation]\n",
    "        C --> D[Ray Batching]\n",
    "        D --> E[Volume Sampling]\n",
    "        E --> F[Positional Encoding]\n",
    "        F --> G[Network]\n",
    "        G --> H[Color, Density]\n",
    "    end\n",
    "\n",
    "    subgraph Rendering\n",
    "        H --> I[Alpha Blending]\n",
    "        I --> J[Rendered Color]\n",
    "    end\n",
    "\n",
    "    subgraph Training\n",
    "        B -.-> L[Loss Calculation]\n",
    "        J -.-> L\n",
    "        G -.-> L\n",
    "        L --> G\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Description\n",
    "\n",
    "1. Positional Encoding of input coordinates\n",
    "    - For learning high-frequency features\n",
    "    - Using Fourier features\n",
    "2. Stochastic Gradient Descent\n",
    "    - For minimizing the error between the true and rendered images\n",
    "    - Choosing a random image from the dataset each iteration\n",
    "<!-- 3. Hierarchical Sampling\n",
    "    - For high-frequency representions\n",
    "    - Using two networks with different sample size -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Details\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "The raw and encoded coordinate values will be concatenated to form the network input.\n",
    "\n",
    "Each coordinate value in `Position` and `Direction` is encoded as follows:\n",
    "\n",
    "$$\n",
    "Encode_{N}(p) \\\\\n",
    "\n",
    "= \\{\\sin (2^0 \\pi p), \\cos (2^0 \\pi p), \\ldots, \\sin (2^{N-1} \\pi p), \\cos (2^{N-1} \\pi p)\\} \\\\\n",
    "\n",
    "= \\{\\sin (2^0 \\pi p), \\sin (\\frac{\\pi}{2} + 2^0 \\pi p), \\ldots, \\sin (2^{N-1} \\pi p), \\sin (\\frac{\\pi}{2} + 2^{N-1} \\pi p)\\} \\\\\n",
    "\n",
    "\\text{where } p \\in \\mathbb{R}, \\ N \\in \\mathbb{N}, \\ Encode_{N}(p) \\in \\mathbb{R}^{2N}\n",
    "$$\n",
    "\n",
    "The encoded dimensions are calculated as follows:\n",
    "\n",
    "| Input     | Dimension | N   | Encoded Dimension |\n",
    "| --------- | --------- | --- | ----------------- |\n",
    "| Position  | 3         | 10  | $3 (1 + 2N) = 63$ |\n",
    "| Direction | 3         | 4   | $3 (1 + 2N) = 27$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Details (Cont.)\n",
    "\n",
    "### Network Definition\n",
    "\n",
    "The neural network is a multi-layer perceptron (MLP) with the following structure:\n",
    "- The density is not dependent on the direction\n",
    "- The fifth hidden layer concatenates the input as a skip connection\n",
    "\n",
    "```mermaid\n",
    "%%{init: {\n",
    "    \"theme\": \"neutral\",\n",
    "    \"themeVariables\": {\n",
    "        \"fontFamily\": \"Menlo, monospace\",\n",
    "        \"fontSize\": \"10px\"\n",
    "    }\n",
    "}}%%\n",
    "flowchart TD\n",
    "    ip1([Input Position 3])\n",
    "    ep1([Encoded Position 63])\n",
    "    h1([Hidden Layer 256])\n",
    "    h2([Hidden Layer 256])\n",
    "    h3([Hidden Layer 256])\n",
    "    h4([Hidden Layer 256])\n",
    "    h5([Hidden Layer 256])\n",
    "    ip2([Input Position 3])\n",
    "    ep2([Encoded Position 63])\n",
    "    h6([Hidden Layer 256])\n",
    "    h7([Hidden Layer 256])\n",
    "    h8([Hidden Layer 256])\n",
    "    od([Output Density 1])\n",
    "    iof([Input/Output Feature 256])\n",
    "    id([Input Direction 3])\n",
    "    ed([Encoded Direction 27])\n",
    "    ha([Additional Hidden Layer 128])\n",
    "    oc([Output Color 3])\n",
    "\n",
    "    ip1 -->|Encode| ep1\n",
    "    ep1 -->|ReLU| h1\n",
    "    h1 -->|ReLU| h2\n",
    "    h2 -->|ReLU| h3\n",
    "    h3 -->|ReLU| h4\n",
    "    h4 -->|ReLU| h5\n",
    "    ip2 -->|Encode| ep2\n",
    "    ep2 ---|Concatenate| h5\n",
    "    h5 -->|ReLU| h6\n",
    "    h6 -->|ReLU| h7\n",
    "    h7 -->|ReLU| h8\n",
    "    h8 -->|ReLU| od\n",
    "    h8 --> iof\n",
    "    id -->|Encode| ed\n",
    "    ed ---|Concatenate| iof\n",
    "    iof -->|ReLU| ha\n",
    "    ha -->|Sigmoid| oc\n",
    "\n",
    "    style ip1 fill:palegreen\n",
    "    style ip2 fill:palegreen\n",
    "    style id fill:palegreen\n",
    "    style ep2 fill:mediumaquamarine\n",
    "    style ep1 fill:mediumaquamarine\n",
    "    style ed fill:mediumaquamarine\n",
    "    style h1 fill:deepskyblue\n",
    "    style h2 fill:deepskyblue\n",
    "    style h3 fill:deepskyblue\n",
    "    style h4 fill:deepskyblue\n",
    "    style h5 fill:deepskyblue\n",
    "    style h6 fill:deepskyblue\n",
    "    style h7 fill:deepskyblue\n",
    "    style h8 fill:deepskyblue\n",
    "    style ha fill:deepskyblue\n",
    "    style iof fill:tan\n",
    "    style od fill:salmon\n",
    "    style oc fill:salmon\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Details (Cont.)\n",
    "\n",
    "### Volume Sampling\n",
    "\n",
    "To represent a continuous scene, we can sample points along the rays, which can be written as $r(t_{i}) = o + d t_{i}$ where:\n",
    "- $o$ is the origin point\n",
    "- $d$ is the direction vector\n",
    "- $t_{i} \\sim U[\\frac{i - 1}{N}, \\frac{i}{N}]$ is the distance along the ray\n",
    "- $N$ is the number of samples per ray, we use $N = 64$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering Details (Cont.)\n",
    "\n",
    "### Alpha Blending\n",
    "\n",
    "$$\n",
    "\\hat{C} = \\sum_{i=1}^{N} T_{i} \\alpha_{i} c_{i} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Details\n",
    "\n",
    "| Module Name                       | Details                                    |\n",
    "| --------------------------------- | ------------------------------------------ |\n",
    "| Optimizer - Adam                  | Learning rate: $5 \\times 10^{-4}$          |\n",
    "| Loss function - Mean Square Error | Error between the true and rendered colors |\n",
    "| Data loader                       | 1024 rays per batch                        |\n",
    "\n",
    "$$\n",
    "Loss = \\frac{\\Sigma_{r \\in \\mathbb{R}} (C_{rendered}(r) - C_{true}(r))^2}{|\\mathbb{R}|} \\\\\n",
    "\n",
    "\\text{where } \\mathbb{R} \\text{ is a batch of rays}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. View synthesis. (n.d.). In Wikipedia. Retrieved from https://en.wikipedia.org/wiki/View_synthesis\n",
    "2. Neural radiance field. (n.d.). In Wikipedia. Retrieved from https://en.wikipedia.org/wiki/Neural_radiance_field\n",
    "3. Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., & Ng, R. (2020). NeRF: Neural radiance fields for image synthesis. arXiv preprint arXiv:2003.08934. Retrieved from https://arxiv.org/pdf/2003.08934\n",
    "4. Tancik, M., Srinivasan, P. P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J. T., & Ng, R. (2020). Fourier features let networks learn high frequency functions in low dimensional domains. NeurIPS. Retrieved from https://arxiv.org/pdf/2006.10739"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.types import Device\n",
    "\n",
    "\n",
    "class PositionalEncoder(Module):\n",
    "    \"\"\"\n",
    "    ### Dimension Transformations\n",
    "    - Input: `[..., D]`\n",
    "    - Output: `[..., D * (2 * encoding_factor + 1)]`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding_factor: int, device: Device | None = None):\n",
    "        import torch\n",
    "\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "\n",
    "        encoding_factor = max(int(encoding_factor), 0)\n",
    "\n",
    "        freq_lvls = torch.arange(encoding_factor, device=device)\n",
    "        self.freq = ((1 << freq_lvls) * torch.pi).repeat_interleave(2)\n",
    "        sine_offsets = torch.tensor([0.0, torch.pi / 2])\n",
    "        self.offsets = sine_offsets.repeat(encoding_factor)\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        inputs = torch.as_tensor(inputs).unsqueeze(-1)\n",
    "\n",
    "        features = (self.freq * inputs + self.offsets).sin_()\n",
    "        features = torch.concat([inputs, features], dim=-1)\n",
    "        features = features.reshape(*inputs.shape[:-2], -1)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "class NeRF(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_count: int | None = None,\n",
    "        hidden_dim: int | None = None,\n",
    "        additional_hidden_dim: int | None = None,\n",
    "        position_encoding_factor: int | None = None,\n",
    "        direction_encoding_factor: int | None = None,\n",
    "    ):\n",
    "        from torch import nn\n",
    "\n",
    "        super(NeRF, self).__init__()\n",
    "\n",
    "        layer_count = int(layer_count or 8)\n",
    "        hidden_dim = int(hidden_dim or 256)\n",
    "        additional_hidden_dim = int(additional_hidden_dim or hidden_dim // 2)\n",
    "        position_encoding_factor = int(position_encoding_factor or 10)\n",
    "        direction_encoding_factor = int(direction_encoding_factor or 4)\n",
    "\n",
    "        COLOR_DIM = 3\n",
    "        DENSITY_DIM = 1\n",
    "        RAW_POSITION_DIM = 3\n",
    "        RAW_DIRECTION_DIM = 3\n",
    "        encoded_position_dim = RAW_POSITION_DIM * (1 + 2 * position_encoding_factor)\n",
    "        encoded_direction_dim = RAW_DIRECTION_DIM * (1 + 2 * direction_encoding_factor)\n",
    "\n",
    "        self.position_hidden_layer_skip_indexs = set(\n",
    "            [i for i in range(1, layer_count - 1) if i % 4 == 0]\n",
    "        )\n",
    "        self.position_input_layer = nn.Linear(encoded_position_dim, hidden_dim)\n",
    "        self.position_hidden_layers = nn.ModuleList(\n",
    "            [\n",
    "                (\n",
    "                    nn.Linear(hidden_dim + encoded_position_dim, hidden_dim)\n",
    "                    if i in self.position_hidden_layer_skip_indexs\n",
    "                    else nn.Linear(hidden_dim, hidden_dim)\n",
    "                )\n",
    "                for i in range(layer_count)\n",
    "            ]\n",
    "        )\n",
    "        self.density_output_layer = nn.Linear(hidden_dim, DENSITY_DIM)\n",
    "        self.direction_input_layer = nn.Linear(\n",
    "            hidden_dim + encoded_direction_dim,\n",
    "            additional_hidden_dim,\n",
    "        )\n",
    "        self.color_output_layer = nn.Linear(additional_hidden_dim, COLOR_DIM)\n",
    "\n",
    "        self.position_input_encoder = PositionalEncoder(position_encoding_factor)\n",
    "        self.direction_input_encoder = PositionalEncoder(direction_encoding_factor)\n",
    "\n",
    "    def forward(self, inputs: Tensor):\n",
    "        import torch\n",
    "\n",
    "        inputs = torch.as_tensor(inputs)\n",
    "\n",
    "        raw_positions = inputs[..., :3]\n",
    "        raw_directions = inputs[..., 3:]\n",
    "        encoded_positions: Tensor = self.position_input_encoder(raw_positions)\n",
    "        encoded_directions: Tensor = self.direction_input_encoder(raw_directions)\n",
    "\n",
    "        hidden_positions: Tensor = self.position_input_layer(encoded_positions)\n",
    "        for index, layer in enumerate(self.position_hidden_layers):\n",
    "            hidden_positions.relu_()\n",
    "            hidden_positions = layer(\n",
    "                torch.concat([hidden_positions, encoded_positions], dim=-1)\n",
    "                if index in self.position_hidden_layer_skip_indexs\n",
    "                else hidden_positions\n",
    "            )\n",
    "\n",
    "        density: Tensor = self.density_output_layer(hidden_positions).relu_()\n",
    "        hidden_directions: Tensor = self.direction_input_layer(\n",
    "            torch.concat([hidden_positions, encoded_directions], dim=-1)\n",
    "        ).relu_()\n",
    "        color: Tensor = self.color_output_layer(hidden_directions).sigmoid_()\n",
    "\n",
    "        return torch.concat([color, density], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogNormalInitializer:\n",
    "    \"\"\"\n",
    "    ## Examples\n",
    "    ```python\n",
    "    from torch.nn import Module\n",
    "\n",
    "    Module().apply(LogNormalInitializer(mean=0.0, std=2.0, seed=1))\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        mean: float | None = None,\n",
    "        std: float | None = None,\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        mean = float(mean or 0.0)\n",
    "        std = float(std or 2.0)\n",
    "        seed = int(seed or 1)\n",
    "\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.seed = seed\n",
    "\n",
    "    def __call__(self, module: Module) -> None:\n",
    "        import torch\n",
    "\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            if module.weight is not None:\n",
    "                with torch.no_grad():\n",
    "                    module.weight.log_normal_(\n",
    "                        mean=self.mean,\n",
    "                        std=self.std,\n",
    "                        generator=torch.Generator(\n",
    "                            module.weight.device,\n",
    "                        ).manual_seed(\n",
    "                            self.seed,\n",
    "                        ),\n",
    "                    ).clamp_min_(\n",
    "                        torch.finfo(torch.float32).eps,\n",
    "                    )\n",
    "                    module.weight.div_(\n",
    "                        module.weight.max(),\n",
    "                    ).clamp_min_(\n",
    "                        torch.finfo(torch.float32).eps,\n",
    "                    )\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeRF(\n",
       "  (position_input_layer): Linear(in_features=63, out_features=256, bias=True)\n",
       "  (position_hidden_layers): ModuleList(\n",
       "    (0-3): 4 x Linear(in_features=256, out_features=256, bias=True)\n",
       "    (4): Linear(in_features=319, out_features=256, bias=True)\n",
       "    (5-7): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (density_output_layer): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (direction_input_layer): Linear(in_features=283, out_features=128, bias=True)\n",
       "  (color_output_layer): Linear(in_features=128, out_features=3, bias=True)\n",
       "  (position_input_encoder): PositionalEncoder()\n",
       "  (direction_input_encoder): PositionalEncoder()\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NeRF().apply(LogNormalInitializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   348  100   348    0     0   1070      0 --:--:-- --:--:-- --:--:--  1070\n",
      "100 12.1M  100 12.1M    0     0   735k      0  0:00:16  0:00:16 --:--:-- 1884k0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n"
     ]
    }
   ],
   "source": [
    "!curl -OL 'http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((106, 100, 100, 3), (106, 4, 4), array(138.8888789))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load('tiny_nerf_data.npz')\n",
    "images = data['images']\n",
    "poses = data['poses']\n",
    "focal = data['focal']\n",
    "H, W = images.shape[1:3]\n",
    "(images.shape, poses.shape, focal)\n",
    "# H W C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.9990219e-01,  4.1922452e-03, -1.3345719e-02, -5.3798322e-02],\n",
       "       [-1.3988681e-02, -2.9965907e-01,  9.5394367e-01,  3.8454704e+00],\n",
       "       [-4.6566129e-10,  9.5403719e-01,  2.9968831e-01,  1.2080823e+00],\n",
       "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  1.0000000e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Volume Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class VolumeSampler:\n",
    "    \"\"\"\n",
    "    ## Dimension Transformations\n",
    "    - Input: `[4, 4]`\n",
    "    - Output: `[height, width, points_per_ray, 3]`\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        height: int,\n",
    "        width: int,\n",
    "        focal: float,\n",
    "        points_per_ray: int | None = None,\n",
    "    ):\n",
    "        import torch\n",
    "\n",
    "        focal_inverse = 1.0 / float(focal)\n",
    "        unit_half_norm = focal_inverse / 2\n",
    "        height_half_norm = height * unit_half_norm\n",
    "        width_half_norm = width * unit_half_norm\n",
    "        points_per_ray = int(points_per_ray or 96)\n",
    "\n",
    "        self.directions = torch.stack(\n",
    "            torch.meshgrid(\n",
    "                torch.arange(\n",
    "                    -width_half_norm + unit_half_norm,\n",
    "                    width_half_norm,\n",
    "                    focal_inverse,\n",
    "                ),\n",
    "                torch.arange(\n",
    "                    height_half_norm - unit_half_norm,\n",
    "                    -height_half_norm,\n",
    "                    -focal_inverse,\n",
    "                ),\n",
    "                torch.tensor(-1.0),\n",
    "                indexing=\"xy\",\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "        self.distances = (\n",
    "            torch.arange(points_per_ray, dtype=torch.float)\n",
    "            .add_(torch.rand(points_per_ray))\n",
    "            .div_(points_per_ray)\n",
    "            .unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "    def __call__(self, posture: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        posture = torch.as_tensor(posture)[:3]\n",
    "\n",
    "        directions = (self.directions * posture[:, :3]).sum(dim=-1)\n",
    "        origins = posture[:, 3].broadcast_to(directions.shape)\n",
    "        points = origins.unsqueeze(-2) + directions.unsqueeze(-2) * self.distances\n",
    "        return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100, 96, 3])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VolumeSampler(H, W, focal)(poses[0]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
