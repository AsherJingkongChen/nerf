{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @page {\n",
       "        size: A3 landscape;\n",
       "        margin: 0;\n",
       "    }\n",
       "    .jp-Cell:first-child {\n",
       "        display: none;\n",
       "    }\n",
       "    .jp-RenderedMermaid {\n",
       "        justify-content: center;\n",
       "    }\n",
       "    h2 {\n",
       "        page-break-before: always;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    @page {\n",
    "        size: A3 landscape;\n",
    "        margin: 0;\n",
    "    }\n",
    "    .jp-Cell:first-child {\n",
    "        display: none;\n",
    "    }\n",
    "    .jp-RenderedMermaid {\n",
    "        justify-content: center;\n",
    "    }\n",
    "    h2 {\n",
    "        page-break-before: always;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Synthesis\n",
    "\n",
    "> Implement by NeRF in pyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Description\n",
    "\n",
    "\"View synthesis\" is a task which\n",
    "generating images of a 3D scene from a specific point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Description\n",
    "\n",
    "\"NeRF\" (Neural Radiance Field) solved \"View synthesis\"\n",
    "by representing 3D scene using a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description\n",
    "\n",
    "1. Preprocessing\n",
    "2. Inference\n",
    "3. Rendering\n",
    "4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Preprocessing\n",
    "\n",
    "{{ True image }} → {{ Position, Direction, True color }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Inference\n",
    "\n",
    "{{ Position, Direction }} → {{ Volumetric sampling }} → {{ Positional encoding }} → {{ Network }} → {{ Color, Density }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Rendering\n",
    "\n",
    "{{ Color, Density }} → {{ Alpha blending }} → {{ Rendered color }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Training\n",
    "\n",
    "{{ True color, Rendered color, Network }} → {{ Network }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    subgraph Preprocessing\n",
    "        ti[True Image]\n",
    "        cp[Camera Posture]\n",
    "        tc[True Color]\n",
    "    end\n",
    "\n",
    "    subgraph Inference\n",
    "        vs[Volume Sampling]\n",
    "        B --> C[Ray Generation]\n",
    "        C --> D[Ray Batching]\n",
    "        D --> E[Volume Sampling]\n",
    "        E --> F[Positional Encoding]\n",
    "        F --> G[Network]\n",
    "        G --> H[Color, Density]\n",
    "    end\n",
    "\n",
    "    subgraph Rendering\n",
    "        H --> I[Alpha Blending]\n",
    "        I --> J[Rendered Color]\n",
    "    end\n",
    "\n",
    "    subgraph Training\n",
    "        B -.-> L[Loss Calculation]\n",
    "        J -.-> L\n",
    "        G -.-> L\n",
    "        L --> G\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Description\n",
    "\n",
    "1. Positional Encoding of input coordinates\n",
    "    - For learning high-frequency features\n",
    "    - Using Fourier features\n",
    "2. Stochastic Gradient Descent\n",
    "    - For minimizing the error between the true and rendered images\n",
    "    - Choosing a random image from the dataset each iteration\n",
    "<!-- 3. Hierarchical Sampling\n",
    "    - For high-frequency representions\n",
    "    - Using two networks with different sample size -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Details\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "The raw and encoded coordinate values will be concatenated to form the network input.\n",
    "\n",
    "Each coordinate value in `Position` and `Direction` is encoded as follows:\n",
    "\n",
    "$$\n",
    "Encode_{N}(p) \\\\\n",
    "\n",
    "= \\{\\sin (2^0 \\pi p), \\cos (2^0 \\pi p), \\ldots, \\sin (2^{N-1} \\pi p), \\cos (2^{N-1} \\pi p)\\} \\\\\n",
    "\n",
    "= \\{\\sin (2^0 \\pi p), \\sin (\\frac{\\pi}{2} + 2^0 \\pi p), \\ldots, \\sin (2^{N-1} \\pi p), \\sin (\\frac{\\pi}{2} + 2^{N-1} \\pi p)\\} \\\\\n",
    "\n",
    "\\text{where } p \\in \\mathbb{R}, \\ N \\in \\mathbb{N}, \\ Encode_{N}(p) \\in \\mathbb{R}^{2N}\n",
    "$$\n",
    "\n",
    "The encoded dimensions are calculated as follows:\n",
    "\n",
    "| Input     | Dimension | N   | Encoded Dimension |\n",
    "| --------- | --------- | --- | ----------------- |\n",
    "| Position  | 3         | 10  | $3 (1 + 2N) = 63$ |\n",
    "| Direction | 3         | 4   | $3 (1 + 2N) = 27$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Details (Cont.)\n",
    "\n",
    "### Network Definition\n",
    "\n",
    "The neural network is a multi-layer perceptron (MLP) with the following structure:\n",
    "- The density is not dependent on the direction\n",
    "- The fifth hidden layer concatenates the input as a skip connection\n",
    "\n",
    "```mermaid\n",
    "%%{init: {\n",
    "    \"theme\": \"neutral\",\n",
    "    \"themeVariables\": {\n",
    "        \"fontFamily\": \"Menlo, monospace\",\n",
    "        \"fontSize\": \"10px\"\n",
    "    }\n",
    "}}%%\n",
    "flowchart TD\n",
    "    ip1([Input Position 3])\n",
    "    ep1([Encoded Position 63])\n",
    "    h1([Hidden Layer 256])\n",
    "    h2([Hidden Layer 256])\n",
    "    h3([Hidden Layer 256])\n",
    "    h4([Hidden Layer 256])\n",
    "    h5([Hidden Layer 256])\n",
    "    ip2([Input Position 3])\n",
    "    ep2([Encoded Position 63])\n",
    "    h6([Hidden Layer 256])\n",
    "    h7([Hidden Layer 256])\n",
    "    h8([Hidden Layer 256])\n",
    "    od([Output Density 1])\n",
    "    iof([Input/Output Feature 256])\n",
    "    id([Input Direction 3])\n",
    "    ed([Encoded Direction 27])\n",
    "    ha([Additional Hidden Layer 128])\n",
    "    oc([Output Color 3])\n",
    "\n",
    "    ip1 -->|Encode| ep1\n",
    "    ep1 -->|ReLU| h1\n",
    "    h1 -->|ReLU| h2\n",
    "    h2 -->|ReLU| h3\n",
    "    h3 -->|ReLU| h4\n",
    "    h4 -->|ReLU| h5\n",
    "    ip2 -->|Encode| ep2\n",
    "    ep2 ---|Concatenate| h5\n",
    "    h5 -->|ReLU| h6\n",
    "    h6 -->|ReLU| h7\n",
    "    h7 -->|ReLU| h8\n",
    "    h8 -->|ReLU| od\n",
    "    h8 --> iof\n",
    "    id -->|Encode| ed\n",
    "    ed ---|Concatenate| iof\n",
    "    iof -->|ReLU| ha\n",
    "    ha -->|Sigmoid| oc\n",
    "\n",
    "    style ip1 fill:palegreen\n",
    "    style ip2 fill:palegreen\n",
    "    style id fill:palegreen\n",
    "    style ep2 fill:mediumaquamarine\n",
    "    style ep1 fill:mediumaquamarine\n",
    "    style ed fill:mediumaquamarine\n",
    "    style h1 fill:deepskyblue\n",
    "    style h2 fill:deepskyblue\n",
    "    style h3 fill:deepskyblue\n",
    "    style h4 fill:deepskyblue\n",
    "    style h5 fill:deepskyblue\n",
    "    style h6 fill:deepskyblue\n",
    "    style h7 fill:deepskyblue\n",
    "    style h8 fill:deepskyblue\n",
    "    style ha fill:deepskyblue\n",
    "    style iof fill:tan\n",
    "    style od fill:salmon\n",
    "    style oc fill:salmon\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Details (Cont.)\n",
    "\n",
    "### Volume Sampling\n",
    "\n",
    "To represent a continuous scene, we can sample points along the rays, which can be written as $r(t_{i}) = o + d t_{i}$ where:\n",
    "- $o$ is the origin point\n",
    "- $d$ is the direction vector\n",
    "- $t_{i} \\sim U[\\frac{i - 1}{N}, \\frac{i}{N}]$ is the distance along the ray (Stratified sampling)\n",
    "- $N$ is the number of samples per ray, we use $N = 96$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering Details (Cont.)\n",
    "\n",
    "### Volume Rendering\n",
    "\n",
    "$$\n",
    "\\hat{C} = \\sum_{i=1}^{N} T_{i} \\alpha_{i} c_{i} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Details\n",
    "\n",
    "| Module Name                       | Details                                    |\n",
    "| --------------------------------- | ------------------------------------------ |\n",
    "| Optimizer - Adam                  | Learning rate is $5 \\times 10^{-4}$        |\n",
    "| Loss function - Mean Square Error | Error between the true and rendered colors |\n",
    "| Data loader                       | 1024 rays per batch to reduce memory cost  |\n",
    "\n",
    "$$\n",
    "Loss = \\frac{\\Sigma_{r \\in \\mathbb{R}} (C_{rendered}(r) - C_{true}(r))^2}{|\\mathbb{R}|} \\\\\n",
    "\n",
    "\\text{where } \\mathbb{R} \\text{ is a batch of rays}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. View synthesis. (n.d.). In Wikipedia. Retrieved from https://en.wikipedia.org/wiki/View_synthesis\n",
    "2. Neural radiance field. (n.d.). In Wikipedia. Retrieved from https://en.wikipedia.org/wiki/Neural_radiance_field\n",
    "3. Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., & Ng, R. (2020). NeRF: Neural radiance fields for image synthesis. arXiv preprint arXiv:2003.08934. Retrieved from https://arxiv.org/pdf/2003.08934\n",
    "4. Tancik, M., Srinivasan, P. P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J. T., & Ng, R. (2020). Fourier features let networks learn high frequency functions in low dimensional domains. NeurIPS. Retrieved from https://arxiv.org/pdf/2006.10739"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.types import Device\n",
    "\n",
    "\n",
    "class PositionalEncoder(Module):\n",
    "    \"\"\"\n",
    "    ## Arguments\n",
    "    - `encoding_factor`: `int`\n",
    "\n",
    "    ## Inputs\n",
    "    1. `Vectors`: `[..., dim]`\n",
    "\n",
    "    ## Outputs\n",
    "    1. `Features`: `[..., dim * (2 * encoding_factor + 1)]`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding_factor: int, device: Device | None = None):\n",
    "        import torch\n",
    "\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "\n",
    "        encoding_factor = max(int(encoding_factor), 0)\n",
    "\n",
    "        freq_lvls = torch.arange(encoding_factor, device=device)\n",
    "        self.freq = ((1 << freq_lvls) * torch.pi).repeat_interleave(2)\n",
    "        sine_offsets = torch.tensor([0.0, torch.pi / 2])\n",
    "        self.offsets = sine_offsets.repeat(encoding_factor)\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        inputs = torch.as_tensor(inputs).unsqueeze(-1)\n",
    "\n",
    "        features = (self.freq * inputs + self.offsets).sin_()\n",
    "        features = torch.concat([inputs, features], dim=-1)\n",
    "        features = features.reshape(*inputs.shape[:-2], -1)\n",
    "        return features\n",
    "\n",
    "    def get_last_dim(self, input_dim: int) -> int:\n",
    "        return int(input_dim) * (self.freq.shape[0] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "class MLP_NeRF(Module):\n",
    "    \"\"\"\n",
    "    ## Inputs\n",
    "    1. `positions`: `[..., 3]`\n",
    "    2. `directions`: `[..., 3]`\n",
    "\n",
    "    ## Outputs\n",
    "    1. `colors`: `[..., 3]`\n",
    "    2. `densities`: `[..., 1]`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_count: int | None = None,\n",
    "        hidden_dim: int | None = None,\n",
    "        additional_hidden_dim: int | None = None,\n",
    "        position_encoder: PositionalEncoder | None = None,\n",
    "        direction_encoder: PositionalEncoder | None = None,\n",
    "    ):\n",
    "        from torch import nn\n",
    "\n",
    "        super(MLP_NeRF, self).__init__()\n",
    "\n",
    "        layer_count = int(layer_count or 8)\n",
    "        hidden_dim = int(hidden_dim or 256)\n",
    "        additional_hidden_dim = int(additional_hidden_dim or hidden_dim // 2)\n",
    "        if position_encoder is None:\n",
    "            position_encoder = PositionalEncoder(10)\n",
    "        if direction_encoder is None:\n",
    "            direction_encoder = PositionalEncoder(4)\n",
    "\n",
    "        COLOR_DIM = 3\n",
    "        DENSITY_DIM = 1\n",
    "        POSITION_DIM = 3\n",
    "        DIRECTION_DIM = 3\n",
    "        encoded_position_dim = position_encoder.get_last_dim(POSITION_DIM)\n",
    "        encoded_direction_dim = direction_encoder.get_last_dim(DIRECTION_DIM)\n",
    "\n",
    "        self.position_hidden_layer_skip_indexs = set(\n",
    "            [i for i in range(1, layer_count - 1) if i % 4 == 0]\n",
    "        )\n",
    "        self.position_input_layer = nn.Linear(encoded_position_dim, hidden_dim)\n",
    "        self.position_hidden_layers = nn.ModuleList(\n",
    "            [\n",
    "                (\n",
    "                    nn.Linear(hidden_dim + encoded_position_dim, hidden_dim)\n",
    "                    if i in self.position_hidden_layer_skip_indexs\n",
    "                    else nn.Linear(hidden_dim, hidden_dim)\n",
    "                )\n",
    "                for i in range(layer_count)\n",
    "            ]\n",
    "        )\n",
    "        self.density_output_layer = nn.Linear(hidden_dim, DENSITY_DIM)\n",
    "        self.direction_input_layer = nn.Linear(\n",
    "            hidden_dim + encoded_direction_dim,\n",
    "            additional_hidden_dim,\n",
    "        )\n",
    "        self.color_output_layer = nn.Linear(additional_hidden_dim, COLOR_DIM)\n",
    "\n",
    "        self.position_input_encoder = position_encoder\n",
    "        self.direction_input_encoder = direction_encoder\n",
    "\n",
    "    def forward(self, positions: Tensor, directions: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        import torch\n",
    "\n",
    "        positions = torch.as_tensor(positions)\n",
    "        directions = torch.as_tensor(directions)\n",
    "\n",
    "        encoded_positions: Tensor = self.position_input_encoder(positions)\n",
    "        encoded_directions: Tensor = self.direction_input_encoder(directions)\n",
    "\n",
    "        hidden_positions: Tensor = self.position_input_layer(encoded_positions)\n",
    "        for index, layer in enumerate(self.position_hidden_layers):\n",
    "            hidden_positions.relu_()\n",
    "            hidden_positions = layer(\n",
    "                torch.concat([hidden_positions, encoded_positions], dim=-1)\n",
    "                if index in self.position_hidden_layer_skip_indexs\n",
    "                else hidden_positions\n",
    "            )\n",
    "\n",
    "        density: Tensor = self.density_output_layer(hidden_positions).relu_()\n",
    "        hidden_directions: Tensor = self.direction_input_layer(\n",
    "            torch.concat([hidden_positions, encoded_directions], dim=-1)\n",
    "        ).relu_()\n",
    "        color: Tensor = self.color_output_layer(hidden_directions).sigmoid_()\n",
    "\n",
    "        return color, density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogNormalInitializer:\n",
    "    \"\"\"\n",
    "    ## Examples\n",
    "    ```python\n",
    "    from torch.nn import Module\n",
    "\n",
    "    Module().apply(LogNormalInitializer(mean=0.0, std=2.0, seed=1))\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mean: float | None = None,\n",
    "        std: float | None = None,\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        from torch import Generator\n",
    "\n",
    "        mean = float(mean or 0.0)\n",
    "        std = float(std or 2.0)\n",
    "        if seed is not None:\n",
    "            seed = int(seed)\n",
    "\n",
    "        self.generator: Generator = seed\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, module: Module) -> None:\n",
    "        import torch\n",
    "\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            if module.weight is not None:\n",
    "                with torch.no_grad():\n",
    "                    if type(self.generator) is int:\n",
    "                        self.generator = torch.Generator(\n",
    "                            module.weight.device,\n",
    "                        ).manual_seed(\n",
    "                            self.generator,\n",
    "                        )\n",
    "                    epsilon = torch.finfo(torch.float).eps\n",
    "                    module.weight.log_normal_(\n",
    "                        mean=self.mean,\n",
    "                        std=self.std,\n",
    "                        generator=self.generator,\n",
    "                    ).clamp_min_(\n",
    "                        epsilon,\n",
    "                    )\n",
    "                    module.weight.div_(\n",
    "                        module.weight.max(),\n",
    "                    ).clamp_min_(\n",
    "                        epsilon,\n",
    "                    )\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Ray Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.types import Device\n",
    "\n",
    "\n",
    "class RayGenerator:\n",
    "    \"\"\"\n",
    "    ## Arguments\n",
    "    - `height`: `int`\n",
    "    - `width`: `int`\n",
    "    - `focal`: `float`\n",
    "\n",
    "    ## Inputs\n",
    "    1. `posture`: `[4, 4]`\n",
    "\n",
    "    ## Outputs\n",
    "    1. `origins`: `[height, width, 3]`\n",
    "    2. `directions`: `[height, width, 3]`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        height: int,\n",
    "        width: int,\n",
    "        focal: float,\n",
    "        device: Device | None = None,\n",
    "    ):\n",
    "        import torch\n",
    "\n",
    "        focal_inverse = 1.0 / float(focal)\n",
    "        unit_half_norm = focal_inverse / 2\n",
    "        height_half_norm = height * unit_half_norm\n",
    "        width_half_norm = width * unit_half_norm\n",
    "\n",
    "        self.directions = torch.stack(\n",
    "            torch.meshgrid(\n",
    "                torch.arange(\n",
    "                    -width_half_norm + unit_half_norm,\n",
    "                    width_half_norm,\n",
    "                    focal_inverse,\n",
    "                    device=device,\n",
    "                ),\n",
    "                torch.arange(\n",
    "                    height_half_norm - unit_half_norm,\n",
    "                    -height_half_norm,\n",
    "                    -focal_inverse,\n",
    "                    device=device,\n",
    "                ),\n",
    "                torch.tensor(-1.0),\n",
    "                indexing=\"xy\",\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "    def __call__(self, posture: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        posture = torch.as_tensor(posture)[:3]\n",
    "\n",
    "        directions = (self.directions * posture[:, :3]).sum(dim=-1)\n",
    "        directions.div_(directions.norm(dim=-1, keepdim=True))\n",
    "        origins = posture[:, 3].broadcast_to(directions.shape)\n",
    "        return origins, directions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Stratified Point Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StratifiedPointSampler:\n",
    "    \"\"\"\n",
    "    ## Arguments\n",
    "    - `points_per_ray`: `int`\n",
    "\n",
    "    ## Inputs\n",
    "    1. `origins`: `[..., 3]`\n",
    "    2. `directions`: `[..., 3]`\n",
    "\n",
    "    ## Outputs\n",
    "    1. `points`: `[..., points_per_ray, 3]`\n",
    "    2. `intervals`: `[points_per_ray]` (Ended with `torch.finfo(torch.float).max`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, points_per_ray: int | None = None, seed: int | None = None):\n",
    "        from torch import Generator\n",
    "\n",
    "        points_per_ray = int(points_per_ray or 96)\n",
    "        if seed is not None:\n",
    "            seed = int(seed)\n",
    "\n",
    "        self.generator: Generator = seed\n",
    "        self.points_per_ray = points_per_ray\n",
    "\n",
    "    def __call__(self, origins: Tensor, directions: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        origins = torch.as_tensor(origins).unsqueeze(-2)\n",
    "        directions = torch.as_tensor(directions).unsqueeze(-2)\n",
    "\n",
    "        device = origins.device\n",
    "        if type(self.generator) is int:\n",
    "            self.generator = torch.Generator(device).manual_seed(self.generator)\n",
    "\n",
    "        distances = (\n",
    "            torch.arange(\n",
    "                self.points_per_ray,\n",
    "                device=device,\n",
    "                dtype=torch.float,\n",
    "            )\n",
    "            .add_(\n",
    "                torch.rand(\n",
    "                    self.points_per_ray,\n",
    "                    device=device,\n",
    "                    generator=self.generator,\n",
    "                )\n",
    "            )\n",
    "            .div_(self.points_per_ray)\n",
    "        )\n",
    "        points = origins + directions * distances.unsqueeze(-1)\n",
    "        intervals = distances[..., 1:] - distances[..., :-1]\n",
    "        intervals = torch.concat(\n",
    "            [intervals, torch.tensor([torch.finfo(torch.float).max])], dim=-1\n",
    "        )\n",
    "        return points, intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((106, 100, 100, 3), (106, 4, 4), array(138.8888789))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load('tiny_nerf_data.npz')\n",
    "images = data['images']\n",
    "poses = data['poses']\n",
    "focal = data['focal']\n",
    "H, W = images.shape[1:3]\n",
    "(images.shape, poses.shape, focal)\n",
    "# H W C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch import Tensor\n",
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    focal: float\n",
    "    images: Tensor\n",
    "    postures: Tensor\n",
    "    size: int\n",
    "\n",
    "    def __init__(self, url: str, seed: int | None = None) -> None:\n",
    "        from httpx import get\n",
    "        from io import BytesIO\n",
    "        from numpy import load\n",
    "        from random import seed as set_seed\n",
    "        import torch\n",
    "\n",
    "        try:\n",
    "            file = BytesIO(\n",
    "                get(\n",
    "                    url,\n",
    "                    follow_redirects=True,\n",
    "                    timeout=60,\n",
    "                )\n",
    "                .raise_for_status()\n",
    "                .content\n",
    "            )\n",
    "        except:\n",
    "            file = open(url, \"rb\")\n",
    "\n",
    "        with file as file_entered:\n",
    "            arrays = load(file_entered)\n",
    "            self.focal = float(arrays[\"focal\"])\n",
    "            self.images = torch.as_tensor(arrays[\"images\"])\n",
    "            self.postures = torch.as_tensor(arrays[\"poses\"])\n",
    "\n",
    "            if self.images.shape[0] != self.postures.shape[0]:\n",
    "                raise ValueError(\"The number of images and postures must be the same.\")\n",
    "\n",
    "            self.size = self.images.shape[0]\n",
    "\n",
    "        if seed is not None:\n",
    "            seed = int(seed)\n",
    "\n",
    "        set_seed(seed)\n",
    "\n",
    "    def get_image_and_posture(self, index: int | None = None) -> tuple[Tensor, Tensor]:\n",
    "        from random import randint\n",
    "\n",
    "        if index is not None:\n",
    "            index = int(index)\n",
    "        else:\n",
    "            index = randint(0, self.size - 1)\n",
    "\n",
    "        return self.images[index], self.postures[index]\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        repr = f\"{self.__class__.__name__}(\"\n",
    "        for name, value in self.__dict__.items():\n",
    "            if isinstance(value, Tensor):\n",
    "                value = f\"Tensor(shape={tuple(value.shape)}, dtype={value.dtype})\"\n",
    "            elif type(value) is float:\n",
    "                value = f\"{value:.7f}\"\n",
    "            repr += f\"\\n  {name}={value},\"\n",
    "        repr += \"\\n)\"\n",
    "        return repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(\n",
       "  focal=138.8888789,\n",
       "  images=Tensor(shape=(106, 100, 100, 3), dtype=torch.float32),\n",
       "  postures=Tensor(shape=(106, 4, 4), dtype=torch.float32),\n",
       "  size=106,\n",
       ")"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset(\"tiny_nerf_data.npz\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=100x100>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "Image.fromarray((dataset.get_image_and_posture()[0] * 255).type(torch.uint8).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 100, 3]),\n",
       " torch.Size([100, 100, 3]),\n",
       " torch.Size([100, 100, 96, 3]),\n",
       " torch.Size([96]))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raygen = RayGenerator(height=H, width=W, focal=focal)\n",
    "raysampler = StratifiedPointSampler(96)\n",
    "origins, directions = raygen(posture=poses[0])\n",
    "points, intervals = raysampler(origins, directions)\n",
    "origins.shape, directions.shape, points.shape, intervals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 3])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directions.unsqueeze(-2).broadcast_to(points.shape)[0, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02,\n",
       "        1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02,\n",
       "        1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02,\n",
       "        1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02,\n",
       "        1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02,\n",
       "        1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02,\n",
       "        1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02,\n",
       "        1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02,\n",
       "        1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02,\n",
       "        1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02,\n",
       "        1.5873e-02, 1.5873e-02, 1.5873e-02, 3.4028e+38])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as tf\n",
    "z_vals = tf.linspace(0, 1, 64)\n",
    "intv = tf.concat([z_vals[..., 1:] - z_vals[..., :-1], tf.tensor([torch.finfo(torch.float).max])], -1)\n",
    "# alpha = 1.-tf.exp(-sigma_a * dists)  \n",
    "# weights = alpha * tf.math.cumprod(1.-alpha + 1e-10, -1, exclusive=True)\n",
    "intv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
