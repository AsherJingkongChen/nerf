{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @page {\n",
       "        size: A3 landscape;\n",
       "        margin: 0;\n",
       "    }\n",
       "    .jp-Cell:first-child {\n",
       "        display: none;\n",
       "    }\n",
       "    .jp-RenderedMermaid {\n",
       "        justify-content: center;\n",
       "    }\n",
       "    h2 {\n",
       "        page-break-before: always;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    @page {\n",
    "        size: A3 landscape;\n",
    "        margin: 0;\n",
    "    }\n",
    "    .jp-Cell:first-child {\n",
    "        display: none;\n",
    "    }\n",
    "    .jp-RenderedMermaid {\n",
    "        justify-content: center;\n",
    "    }\n",
    "    h2 {\n",
    "        page-break-before: always;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Synthesis\n",
    "\n",
    "> Implement by NeRF in pyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Description\n",
    "\n",
    "\"View synthesis\" is a task which\n",
    "generating images of a 3D scene from a specific point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Description\n",
    "\n",
    "\"NeRF\" (Neural Radiance Field) solved \"View synthesis\"\n",
    "by representing 3D scene using a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description\n",
    "\n",
    "1. Preprocessing\n",
    "2. Inference\n",
    "3. Rendering\n",
    "4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Preprocessing\n",
    "\n",
    "{{ True image }} → {{ Position, Direction, True color }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Inference\n",
    "\n",
    "{{ Position, Direction }} → {{ Volumetric sampling }} → {{ Positional encoding }} → {{ Network }} → {{ Color, Density }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Rendering\n",
    "\n",
    "{{ Color, Density }} → {{ Alpha blending }} → {{ Rendered color }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Training\n",
    "\n",
    "{{ True color, Rendered color, Network }} → {{ Network }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    subgraph Preprocessing\n",
    "        ti[True Image]\n",
    "        cp[Camera Posture]\n",
    "        tc[True Color]\n",
    "    end\n",
    "\n",
    "    subgraph Inference\n",
    "        vs[Volume Sampling]\n",
    "        B --> C[Ray Generation]\n",
    "        C --> D[Ray Batching]\n",
    "        D --> E[Volume Sampling]\n",
    "        E --> F[Positional Encoding]\n",
    "        F --> G[Network]\n",
    "        G --> H[Color, Density]\n",
    "    end\n",
    "\n",
    "    subgraph Rendering\n",
    "        H --> I[Alpha Blending]\n",
    "        I --> J[Rendered Color]\n",
    "    end\n",
    "\n",
    "    subgraph Training\n",
    "        B -.-> L[Loss Calculation]\n",
    "        J -.-> L\n",
    "        G -.-> L\n",
    "        L --> G\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Description\n",
    "\n",
    "1. Positional Encoding of input coordinates\n",
    "    - For learning high-frequency features\n",
    "    - Using Fourier features\n",
    "2. Stochastic Gradient Descent\n",
    "    - For minimizing the error between the true and rendered images\n",
    "    - Choosing a random image from the dataset each iteration\n",
    "<!-- 3. Hierarchical Sampling\n",
    "    - For high-frequency representions\n",
    "    - Using two networks with different sample size -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Details\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "The raw and encoded coordinate values will be concatenated to form the network input.\n",
    "\n",
    "Each coordinate value in `Position` and `Direction` is encoded as follows:\n",
    "\n",
    "$$\n",
    "Encode_{N}(p) \\\\\n",
    "\n",
    "= \\{\\sin (2^0 \\pi p), \\cos (2^0 \\pi p), \\ldots, \\sin (2^{N-1} \\pi p), \\cos (2^{N-1} \\pi p)\\} \\\\\n",
    "\n",
    "= \\{\\sin (2^0 \\pi p), \\sin (\\frac{\\pi}{2} + 2^0 \\pi p), \\ldots, \\sin (2^{N-1} \\pi p), \\sin (\\frac{\\pi}{2} + 2^{N-1} \\pi p)\\} \\\\\n",
    "\n",
    "\\text{where } p \\in \\mathbb{R}, \\ N \\in \\mathbb{N}, \\ Encode_{N}(p) \\in \\mathbb{R}^{2N}\n",
    "$$\n",
    "\n",
    "The encoded dimensions are calculated as follows:\n",
    "\n",
    "| Input     | Dimension | N   | Encoded Dimension |\n",
    "| --------- | --------- | --- | ----------------- |\n",
    "| Position  | 3         | 10  | $3 (1 + 2N) = 63$ |\n",
    "| Direction | 3         | 4   | $3 (1 + 2N) = 27$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Details (Cont.)\n",
    "\n",
    "### Neural Prediction\n",
    "\n",
    "The neural network is a multi-layer perceptron (MLP) with the following structure:\n",
    "- The density is not dependent on the direction\n",
    "- The fifth hidden layer concatenates the input as a skip connection\n",
    "\n",
    "```mermaid\n",
    "%%{init: {\n",
    "    \"theme\": \"neutral\",\n",
    "    \"themeVariables\": {\n",
    "        \"fontFamily\": \"Menlo, monospace\",\n",
    "        \"fontSize\": \"10px\"\n",
    "    }\n",
    "}}%%\n",
    "flowchart TD\n",
    "    ip1([Input Position 3])\n",
    "    ep1([Encoded Position 63])\n",
    "    h1([Hidden Layer 256])\n",
    "    h2([Hidden Layer 256])\n",
    "    h3([Hidden Layer 256])\n",
    "    h4([Hidden Layer 256])\n",
    "    h5([Hidden Layer 256])\n",
    "    ip2([Input Position 3])\n",
    "    ep2([Encoded Position 63])\n",
    "    h6([Hidden Layer 256])\n",
    "    h7([Hidden Layer 256])\n",
    "    h8([Hidden Layer 256])\n",
    "    od([Output Density 1])\n",
    "    iof([Input/Output Feature 256])\n",
    "    id([Input Direction 3])\n",
    "    ed([Encoded Direction 27])\n",
    "    ha([Additional Hidden Layer 128])\n",
    "    oc([Output Color 3])\n",
    "\n",
    "    ip1 -->|Encode| ep1\n",
    "    ep1 -->|ReLU| h1\n",
    "    h1 -->|ReLU| h2\n",
    "    h2 -->|ReLU| h3\n",
    "    h3 -->|ReLU| h4\n",
    "    h4 -->|ReLU| h5\n",
    "    ip2 -->|Encode| ep2\n",
    "    ep2 ---|Concatenate| h5\n",
    "    h5 -->|ReLU| h6\n",
    "    h6 -->|ReLU| h7\n",
    "    h7 -->|ReLU| h8\n",
    "    h8 -->|ReLU| od\n",
    "    h8 --> iof\n",
    "    id -->|Encode| ed\n",
    "    ed ---|Concatenate| iof\n",
    "    iof -->|ReLU| ha\n",
    "    ha -->|Sigmoid| oc\n",
    "\n",
    "    style ip1 fill:palegreen\n",
    "    style ip2 fill:palegreen\n",
    "    style id fill:palegreen\n",
    "    style ep2 fill:mediumaquamarine\n",
    "    style ep1 fill:mediumaquamarine\n",
    "    style ed fill:mediumaquamarine\n",
    "    style h1 fill:deepskyblue\n",
    "    style h2 fill:deepskyblue\n",
    "    style h3 fill:deepskyblue\n",
    "    style h4 fill:deepskyblue\n",
    "    style h5 fill:deepskyblue\n",
    "    style h6 fill:deepskyblue\n",
    "    style h7 fill:deepskyblue\n",
    "    style h8 fill:deepskyblue\n",
    "    style ha fill:deepskyblue\n",
    "    style iof fill:tan\n",
    "    style od fill:salmon\n",
    "    style oc fill:salmon\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Details (Cont.)\n",
    "\n",
    "### Stratified Point Sampling\n",
    "\n",
    "To represent a continuous scene, we can sample points along the rays, which can be written as $r(t_{i}) = o + d t_{i}$ where:\n",
    "- $o$ is the origin point\n",
    "- $d$ is the direction vector\n",
    "- $t_{i} \\sim U[\\frac{i - 1}{N}, \\frac{i}{N}]$ is the distance along the ray (Stratified sampling)\n",
    "- $N$ is the number of samples per ray, we use $N = 32$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering Details (Cont.)\n",
    "\n",
    "### Volume Rendering\n",
    "\n",
    "$$\n",
    "\\hat{C} = \\sum_{i=1}^{N} T_{i} \\alpha_{i} c_{i} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Details\n",
    "\n",
    "| Module Name                       | Details                                    |\n",
    "| --------------------------------- | ------------------------------------------ |\n",
    "| Weight updater - Adam             | Learning rate is $5 \\times 10^{-4}$        |\n",
    "| Loss function - Mean Square Error | Error between the true and rendered colors |\n",
    "| Data loader                       | 1024 rays per batch to reduce memory cost  |\n",
    "\n",
    "$$\n",
    "Loss = \\frac{\\Sigma_{r \\in \\mathbb{R}} (C_{rendered}(r) - C_{true}(r))^2}{|\\mathbb{R}|} \\\\\n",
    "\n",
    "\\text{where } \\mathbb{R} \\text{ is a batch of rays}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. View synthesis. (n.d.). In Wikipedia. Retrieved from https://en.wikipedia.org/wiki/View_synthesis\n",
    "2. Neural radiance field. (n.d.). In Wikipedia. Retrieved from https://en.wikipedia.org/wiki/Neural_radiance_field\n",
    "3. Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., & Ng, R. (2020). NeRF: Neural radiance fields for image synthesis. arXiv preprint arXiv:2003.08934. Retrieved from https://arxiv.org/pdf/2003.08934\n",
    "4. Tancik, M., Srinivasan, P. P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J. T., & Ng, R. (2020). Fourier features let networks learn high frequency functions in low dimensional domains. NeurIPS. Retrieved from https://arxiv.org/pdf/2006.10739"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.types import Device\n",
    "\n",
    "\n",
    "class PositionalEncoder(Module):\n",
    "    \"\"\"\n",
    "    ## Arguments\n",
    "    - `encoding_factor`: `int`\n",
    "\n",
    "    ## Inputs\n",
    "    1. `Vectors`: `[..., dim]`\n",
    "\n",
    "    ## Outputs\n",
    "    1. `Features`: `[..., dim * (2 * encoding_factor + 1)]`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding_factor: int, device: Device | None = None):\n",
    "        import torch\n",
    "\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "\n",
    "        encoding_factor = max(int(encoding_factor), 0)\n",
    "\n",
    "        freq_lvls = torch.arange(encoding_factor, device=device)\n",
    "        self.freq = ((2 ** freq_lvls) * torch.pi).repeat_interleave(2)\n",
    "        sine_offsets = torch.tensor([0.0, torch.pi / 2], device=device)\n",
    "        self.offsets = sine_offsets.repeat(encoding_factor)\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        inputs = torch.as_tensor(inputs).unsqueeze(-1)\n",
    "\n",
    "        features = (self.freq * inputs + self.offsets).sin()\n",
    "        features = torch.concat([inputs, features], dim=-1)\n",
    "        features = features.reshape(*inputs.shape[:-2], -1)\n",
    "        return features\n",
    "\n",
    "    def get_last_dim(self, input_dim: int) -> int:\n",
    "        return int(input_dim) * (self.freq.shape[0] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Neural Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.types import Device\n",
    "\n",
    "\n",
    "class MLP(Module):\n",
    "    \"\"\"\n",
    "    ## Inputs\n",
    "    1. `positions` + `directions`: `[..., 3 + 3]`\n",
    "\n",
    "    ## Outputs\n",
    "    1. `colors` + `densities`: `[..., 3 + 1]`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_count: int | None = None,\n",
    "        hidden_dim: int | None = None,\n",
    "        additional_hidden_dim: int | None = None,\n",
    "        position_encoder: PositionalEncoder | None = None,\n",
    "        direction_encoder: PositionalEncoder | None = None,\n",
    "        device: Device | None = None,\n",
    "    ):\n",
    "        from torch import nn\n",
    "\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        layer_count = int(layer_count or 8)\n",
    "        hidden_dim = int(hidden_dim or 256)\n",
    "        additional_hidden_dim = int(additional_hidden_dim or hidden_dim // 2)\n",
    "        if position_encoder is None:\n",
    "            position_encoder = PositionalEncoder(10, device=device)\n",
    "        if direction_encoder is None:\n",
    "            direction_encoder = PositionalEncoder(4, device=device)\n",
    "\n",
    "        COLOR_DIM = 3\n",
    "        DENSITY_DIM = 1\n",
    "        POSITION_DIM = 3\n",
    "        DIRECTION_DIM = 3\n",
    "        encoded_position_dim = position_encoder.get_last_dim(POSITION_DIM)\n",
    "        encoded_direction_dim = direction_encoder.get_last_dim(DIRECTION_DIM)\n",
    "\n",
    "        self.position_hidden_layer_skip_indexs = set(\n",
    "            [i for i in range(1, layer_count - 1) if i % 4 == 0]\n",
    "        )\n",
    "        self.position_input_layer = nn.Linear(\n",
    "            encoded_position_dim,\n",
    "            hidden_dim,\n",
    "            device=device,\n",
    "        )\n",
    "        self.position_hidden_layers = nn.ModuleList(\n",
    "            [\n",
    "                (\n",
    "                    nn.Linear(\n",
    "                        hidden_dim + encoded_position_dim,\n",
    "                        hidden_dim,\n",
    "                        device=device,\n",
    "                    )\n",
    "                    if i in self.position_hidden_layer_skip_indexs\n",
    "                    else nn.Linear(\n",
    "                        hidden_dim,\n",
    "                        hidden_dim,\n",
    "                        device=device,\n",
    "                    )\n",
    "                )\n",
    "                for i in range(layer_count)\n",
    "            ]\n",
    "        )\n",
    "        self.density_output_layer = nn.Linear(\n",
    "            hidden_dim,\n",
    "            DENSITY_DIM,\n",
    "            device=device,\n",
    "        )\n",
    "        self.direction_input_layer = nn.Linear(\n",
    "            hidden_dim + encoded_direction_dim,\n",
    "            additional_hidden_dim,\n",
    "            device=device,\n",
    "        )\n",
    "        self.color_output_layer = nn.Linear(\n",
    "            additional_hidden_dim, COLOR_DIM, device=device\n",
    "        )\n",
    "\n",
    "        self.position_input_encoder = position_encoder\n",
    "        self.direction_input_encoder = direction_encoder\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        import torch\n",
    "\n",
    "        inputs = torch.as_tensor(inputs)\n",
    "\n",
    "        positions = inputs[..., 0:3]\n",
    "        directions = inputs[..., 3:6]\n",
    "        encoded_positions: Tensor = self.position_input_encoder(positions)\n",
    "        encoded_directions: Tensor = self.direction_input_encoder(directions)\n",
    "\n",
    "        hidden_positions: Tensor = self.position_input_layer(encoded_positions).relu()\n",
    "        for index, layer in enumerate(self.position_hidden_layers):\n",
    "            hidden_positions = layer(\n",
    "                torch.concat([hidden_positions, encoded_positions], dim=-1)\n",
    "                if index in self.position_hidden_layer_skip_indexs\n",
    "                else hidden_positions\n",
    "            )\n",
    "            hidden_positions = hidden_positions.relu()\n",
    "\n",
    "        density: Tensor = self.density_output_layer(hidden_positions).relu()\n",
    "        hidden_directions: Tensor = self.direction_input_layer(\n",
    "            torch.concat([hidden_positions, encoded_directions], dim=-1)\n",
    "        ).relu()\n",
    "        color: Tensor = self.color_output_layer(hidden_directions).sigmoid()\n",
    "\n",
    "        return torch.concat([color, density], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Weight Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.optimizer import ParamsT\n",
    "\n",
    "\n",
    "class AdamWeightUpdater(Adam):\n",
    "    def __init__(self, parameters: ParamsT, learning_rate: float | None = None):\n",
    "        learning_rate = float(learning_rate or 5e-4)\n",
    "\n",
    "        super(AdamWeightUpdater, self).__init__(parameters, lr=learning_rate)\n",
    "\n",
    "    def __call__(self, loss: Tensor) -> None:\n",
    "        self.zero_grad()\n",
    "        loss.backward()\n",
    "        self.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Ray Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.types import Device\n",
    "\n",
    "\n",
    "class RayGenerator(Module):\n",
    "    \"\"\"\n",
    "    ## Arguments\n",
    "    - `height`: `int`\n",
    "    - `width`: `int`\n",
    "    - `focal`: `float`\n",
    "\n",
    "    ## Inputs\n",
    "    1. `posture`: `[4, 4]`\n",
    "\n",
    "    ## Outputs\n",
    "    1. `origins`: `[height, width, 3]`\n",
    "    2. `directions`: `[height, width, 3]`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        height: int,\n",
    "        width: int,\n",
    "        focal: float,\n",
    "        device: Device | None = None,\n",
    "    ):\n",
    "        super(RayGenerator, self).__init__()\n",
    "\n",
    "        import torch\n",
    "\n",
    "        focal = float(focal)\n",
    "        height = float(height)\n",
    "        width = float(width)\n",
    "\n",
    "        self.directions = torch.stack(\n",
    "            torch.meshgrid(\n",
    "                (torch.arange(width, device=device) - width / 2) / focal,\n",
    "                -(torch.arange(height, device=device) - height / 2) / focal,\n",
    "                torch.tensor(-1.0, device=device),\n",
    "                indexing=\"xy\",\n",
    "            ),\n",
    "            dim=-1,\n",
    "        ).unsqueeze_(-2)\n",
    "\n",
    "    def forward(self, posture: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        posture = torch.as_tensor(posture)[:3]\n",
    "\n",
    "        directions = (self.directions * posture[:, :3]).sum(dim=-1)\n",
    "        origins = posture[:, 3].broadcast_to(directions.shape)\n",
    "        return origins, directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Stratified Point Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "class StratifiedPointSampler(Module):\n",
    "    \"\"\"\n",
    "    ## Arguments\n",
    "    - `points_per_ray`: `int`\n",
    "\n",
    "    ## Inputs\n",
    "    1. `origins`: `[..., 1, 3]`\n",
    "    2. `directions`: `[..., 1, 3]`\n",
    "\n",
    "    ## Outputs\n",
    "    1. `points`: `[..., points_per_ray, 3]`\n",
    "    2. `intervals`: `[..., points_per_ray]` (Ended with `1e9`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, points_per_ray: int | None = None, seed: int | None = None):\n",
    "        super(StratifiedPointSampler, self).__init__()\n",
    "\n",
    "        from torch import Generator\n",
    "\n",
    "        points_per_ray = int(points_per_ray or 32)\n",
    "        if seed is not None:\n",
    "            seed = int(seed)\n",
    "\n",
    "        self.generator: Generator = seed\n",
    "        self.points_per_ray = points_per_ray\n",
    "\n",
    "    def forward(self, origins: Tensor, directions: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        origins = torch.as_tensor(origins)\n",
    "        directions = torch.as_tensor(directions)\n",
    "\n",
    "        device = origins.device\n",
    "        if type(self.generator) is int:\n",
    "            self.generator = torch.Generator(device).manual_seed(self.generator)\n",
    "\n",
    "        distances = (\n",
    "            torch.linspace(2.0, 6.0, self.points_per_ray, device=device)\n",
    "            .repeat(*origins.shape[:-2], 1)\n",
    "            .add_(\n",
    "                torch.rand(\n",
    "                    *origins.shape[:-2],\n",
    "                    self.points_per_ray,\n",
    "                    device=device,\n",
    "                    generator=self.generator,\n",
    "                ).mul_(4.0 / self.points_per_ray)\n",
    "            )\n",
    "        )\n",
    "        intervals = torch.concat(\n",
    "            [\n",
    "                distances[..., 1:] - distances[..., :-1],\n",
    "                torch.tensor(1e9, device=device).repeat(\n",
    "                    (*origins.shape[:-2], 1),\n",
    "                ),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        points = origins + directions * distances.unsqueeze_(-1)\n",
    "        return points, intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Volume Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "class VolumeRenderer(Module):\n",
    "    \"\"\"\n",
    "    ## Inputs\n",
    "    1. `colors` + `densities`: `[..., points_per_ray, 3 + 1]`\n",
    "    2. `intervals`: `[..., points_per_ray]`\n",
    "\n",
    "    ## Outputs\n",
    "    1. `rendered_colors`: `[..., 3]`\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, inputs: Tensor, intervals: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        inputs = torch.as_tensor(inputs)\n",
    "\n",
    "        colors = inputs[..., :3]\n",
    "        densities = inputs[..., 3]\n",
    "        translucency = (-densities * intervals).exp()\n",
    "        transmittance = (1.0 - translucency) * torch.cumprod(translucency, dim=-1)\n",
    "        rendered_colors = (transmittance.unsqueeze(-1) * colors).sum(dim=-2)\n",
    "        return rendered_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class MSELossFunction:\n",
    "    def __call__(\n",
    "        self,\n",
    "        rendered_color: Tensor,\n",
    "        target_color: Tensor,\n",
    "    ) -> Tensor:\n",
    "        from torch import as_tensor\n",
    "\n",
    "        rendered_color = as_tensor(rendered_color)\n",
    "        target_color = as_tensor(target_color)\n",
    "\n",
    "        loss = (rendered_color - target_color).square().mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "class DataLoader(Module):\n",
    "    def __init__(self, batch_size: int | None = None):\n",
    "        super(DataLoader, self).__init__()\n",
    "\n",
    "        batch_size = int(batch_size or 1)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Iterator[Tensor]:\n",
    "        from torch.utils.data import DataLoader as _DataLoader\n",
    "\n",
    "        return iter(_DataLoader(inputs, batch_size=self.batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch import Tensor\n",
    "from torch.types import Device\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ViewSynthesisDataset:\n",
    "    count: int\n",
    "    focal: float\n",
    "    height: int\n",
    "    images: Tensor\n",
    "    postures: Tensor\n",
    "    width: int\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if self.images.shape[0] != self.postures.shape[0]:\n",
    "            raise ValueError(\"The number of images and postures must be the same\")\n",
    "\n",
    "    @staticmethod\n",
    "    def from_numpy(url: str) -> \"ViewSynthesisDataset\":\n",
    "        from httpx import get\n",
    "        from io import BytesIO\n",
    "        from numpy import load\n",
    "\n",
    "        import torch\n",
    "\n",
    "        try:\n",
    "            file = BytesIO(\n",
    "                get(url, follow_redirects=True, timeout=60).raise_for_status().content\n",
    "            )\n",
    "        except:\n",
    "            file = open(url, \"rb\")\n",
    "\n",
    "        with file as file_entered:\n",
    "            arrays = load(file_entered)\n",
    "            focal = float(arrays[\"focal\"])\n",
    "            images = torch.as_tensor(arrays[\"images\"])\n",
    "            postures = torch.as_tensor(arrays[\"poses\"])\n",
    "\n",
    "        return ViewSynthesisDataset(\n",
    "            count=images.shape[0],\n",
    "            focal=focal,\n",
    "            height=images.shape[1],\n",
    "            images=images,\n",
    "            postures=postures,\n",
    "            width=images.shape[2],\n",
    "        )\n",
    "\n",
    "    def get_image_and_posture(self, index: int | None = None) -> tuple[Tensor, Tensor]:\n",
    "        from random import randint\n",
    "\n",
    "        if index is not None:\n",
    "            index = int(index)\n",
    "        else:\n",
    "            index = randint(0, self.count - 1)\n",
    "\n",
    "        return self.images[index], self.postures[index]\n",
    "\n",
    "    def set_device(self, device: Device) -> \"ViewSynthesisDataset\":\n",
    "        self.images = self.images.to(device)\n",
    "        self.postures = self.postures.to(device)\n",
    "        return self\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        repr = f\"{self.__class__.__name__}(\"\n",
    "        for name, value in self.__dict__.items():\n",
    "            if isinstance(value, Tensor):\n",
    "                value = f\"Tensor(shape={tuple(value.shape)}, dtype={value.dtype})\"\n",
    "            elif type(value) is float:\n",
    "                value = f\"{value:.7f}\"\n",
    "            repr += f\"\\n  {name}={value},\"\n",
    "        repr += \"\\n)\"\n",
    "        return repr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### NeRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.types import Device\n",
    "\n",
    "\n",
    "def display_image(image: Tensor):\n",
    "    from IPython.display import display\n",
    "    from PIL import Image\n",
    "    from torch import uint8\n",
    "\n",
    "    return display(Image.fromarray((image * 255).round().type(uint8).numpy()))\n",
    "\n",
    "\n",
    "class NeRF(Module):\n",
    "    \"\"\"\n",
    "    ## Arguments\n",
    "    - `focal`: `float`\n",
    "    - `height`: `int`\n",
    "    - `width`: `int`\n",
    "\n",
    "    ## Inputs\n",
    "    1. `posture`: `[4, 4]`\n",
    "\n",
    "    ## Outputs\n",
    "    1. `rendered_image`: `[height, width, 3]`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        focal: float,\n",
    "        height: int,\n",
    "        width: int,\n",
    "        points_per_ray: int | None = None,\n",
    "        rays_per_batch: int | None = None,\n",
    "        device: Device | None = None,\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        from random import seed as set_seed\n",
    "\n",
    "        super(NeRF, self).__init__()\n",
    "\n",
    "        points_per_ray = int(points_per_ray or 32)\n",
    "        rays_per_batch = int(rays_per_batch or 1024)\n",
    "        if seed is not None:\n",
    "            seed = int(seed)\n",
    "            set_seed(seed)\n",
    "\n",
    "        points_per_batch = points_per_ray * rays_per_batch\n",
    "\n",
    "        self.generate_rays = RayGenerator(\n",
    "            focal=focal,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            device=device,\n",
    "        )\n",
    "        self.sample = StratifiedPointSampler(\n",
    "            points_per_ray=points_per_ray,\n",
    "            seed=seed,\n",
    "        )\n",
    "        self.load_batches = DataLoader(\n",
    "            batch_size=points_per_batch,\n",
    "        )\n",
    "        self.predict = MLP(\n",
    "            layer_count=8,\n",
    "            hidden_dim=256,\n",
    "            additional_hidden_dim=128,\n",
    "            position_encoder=PositionalEncoder(10, device=device),\n",
    "            direction_encoder=PositionalEncoder(4, device=device),\n",
    "            device=device,\n",
    "        )\n",
    "        self.render = VolumeRenderer()\n",
    "\n",
    "    def forward(self, posture: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        origins: Tensor\n",
    "        directions: Tensor\n",
    "        positions: Tensor\n",
    "        intervals: Tensor\n",
    "\n",
    "        origins, directions = self.generate_rays(posture)\n",
    "        positions, intervals = self.sample(origins, directions)\n",
    "        positions_and_directions = torch.concat(\n",
    "            [\n",
    "                positions,\n",
    "                directions.broadcast_to(positions.shape),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        ).flatten(\n",
    "            end_dim=-2,\n",
    "        )\n",
    "        colors_and_densities = torch.concat(\n",
    "            [\n",
    "                self.predict(batch)\n",
    "                for batch in self.load_batches(positions_and_directions)\n",
    "            ],\n",
    "            dim=0,\n",
    "        ).reshape(\n",
    "            (*positions.shape[:-1], 4),\n",
    "        )\n",
    "        rendered_image = self.render(colors_and_densities, intervals)\n",
    "        # display(\n",
    "        #     dict(\n",
    "        #         origins=origins[0, 0],\n",
    "        #         directions=directions[0, 0],\n",
    "        #         positions=positions[0, 0, 0],\n",
    "        #         intervals=intervals[0, 0],\n",
    "        #         colors_densities=colors_and_densities[0, 0],\n",
    "        #         rendered_image=rendered_image[0, 0],\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "        return rendered_image\n",
    "\n",
    "    @staticmethod\n",
    "    def fit(\n",
    "        dataset: ViewSynthesisDataset,\n",
    "        epochs: int | None = None,\n",
    "        learning_rate: float | None = None,\n",
    "        show_progress: bool | None = None,\n",
    "        *,\n",
    "        points_per_ray: int | None = None,\n",
    "        rays_per_batch: int | None = None,\n",
    "        device: Device | None = None,\n",
    "        seed: int | None = None,\n",
    "    ) -> \"NeRF\":\n",
    "        import torch\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        EPOCHS_PER_DEMO = 50\n",
    "\n",
    "        dataset.set_device(device)\n",
    "        epochs = int(epochs or 1)\n",
    "        if show_progress is None:\n",
    "            show_progress = True\n",
    "\n",
    "        if show_progress:\n",
    "            display_image(dataset.get_image_and_posture(-1)[0])\n",
    "\n",
    "        def weight_initialize(module: Module):\n",
    "            if type(module) is torch.nn.Linear:\n",
    "                with torch.no_grad():\n",
    "                    module.weight.fill_(1e-4)\n",
    "                    module.bias.zero_()\n",
    "\n",
    "        model = NeRF(\n",
    "            focal=dataset.focal,\n",
    "            height=dataset.height,\n",
    "            width=dataset.width,\n",
    "            points_per_ray=points_per_ray,\n",
    "            rays_per_batch=rays_per_batch,\n",
    "            device=device,\n",
    "            seed=seed,\n",
    "        ).apply(\n",
    "            weight_initialize,\n",
    "        )\n",
    "        calculate_loss = MSELossFunction()\n",
    "        update_weight = AdamWeightUpdater(\n",
    "            parameters=model.parameters(),\n",
    "            learning_rate=learning_rate,\n",
    "        )\n",
    "        progress = tqdm(\n",
    "            disable=not show_progress,\n",
    "            desc=f\"Fitting NeRF model to {dataset.count}x images and postures\",\n",
    "            ascii=\"         |\",\n",
    "            colour=\"blue\",\n",
    "            dynamic_ncols=True,\n",
    "            total=epochs,\n",
    "        )\n",
    "\n",
    "        with progress:\n",
    "            for epoch in range(epochs):\n",
    "                image, posture = dataset.get_image_and_posture()\n",
    "                rendered_image: Tensor = model(posture)\n",
    "                loss = calculate_loss(rendered_image, image)\n",
    "                update_weight(loss)\n",
    "\n",
    "                if show_progress and epoch % EPOCHS_PER_DEMO == 0:\n",
    "                    with torch.no_grad():\n",
    "                        posture_demo = dataset.get_image_and_posture(-1)[1]\n",
    "                        rendered_image_demo = model(posture_demo)\n",
    "                        display_image(rendered_image_demo)\n",
    "                        display(rendered_image_demo.max().item())\n",
    "\n",
    "                progress.update()\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = ViewSynthesisDataset.from_numpy(\"http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ViewSynthesisDataset(\n",
       "   count=85,\n",
       "   focal=138.8888789,\n",
       "   height=100,\n",
       "   images=Tensor(shape=(85, 100, 100, 3), dtype=torch.float32),\n",
       "   postures=Tensor(shape=(85, 4, 4), dtype=torch.float32),\n",
       "   width=100,\n",
       " ),\n",
       " ViewSynthesisDataset(\n",
       "   count=21,\n",
       "   focal=138.8888789,\n",
       "   height=100,\n",
       "   images=Tensor(shape=(21, 100, 100, 3), dtype=torch.float32),\n",
       "   postures=Tensor(shape=(21, 4, 4), dtype=torch.float32),\n",
       "   width=100,\n",
       " ))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_count = original_dataset.count // 5\n",
    "test_dataset = ViewSynthesisDataset(\n",
    "    count=test_data_count,\n",
    "    focal=original_dataset.focal,\n",
    "    height=original_dataset.height,\n",
    "    images=original_dataset.images[-test_data_count:],\n",
    "    postures=original_dataset.postures[-test_data_count:],\n",
    "    width=original_dataset.width,\n",
    ")\n",
    "train_dataset = ViewSynthesisDataset(\n",
    "    count=original_dataset.count - test_data_count,\n",
    "    focal=original_dataset.focal,\n",
    "    height=original_dataset.height,\n",
    "    images=original_dataset.images[:-test_data_count],\n",
    "    postures=original_dataset.postures[:-test_data_count],\n",
    "    width=original_dataset.width,\n",
    ")\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=100x100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting NeRF model to 85x images and postures:   0%|\u001b[34m          \u001b[0m| 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABkAGQDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD5/ooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAIAAAD/gAIDAAAAm0lEQVR4Ae3QAQ0AAADCoPdPbQ43iEBhwIABAwYMGDBgwIABAwYMGDBgwIABAwYMGDBgwIABAwYMGDBgwIABAwYMGDBgwIABAwYMGDBgwIABAwYMGDBgwIABAwYMGDBgwIABAwYMGDBgwIABAwYMGDBgwIABAwYMGDBgwIABAwYMGDBgwIABAwYMGDBgwIABAwYMGDBgwICBn4EBdZQAAanO3kQAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=100x100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0012541281757876277"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting NeRF model to 85x images and postures:   3%|\u001b[34m          \u001b[0m| 28/1000 [00:38<21:30,  1.33s/it]"
     ]
    }
   ],
   "source": [
    "model = NeRF.fit(\n",
    "    dataset=train_dataset,\n",
    "    device=\"cpu\",\n",
    "    epochs=1000,\n",
    "    rays_per_batch=2000,\n",
    "    points_per_ray=40,\n",
    "    seed=5336,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
