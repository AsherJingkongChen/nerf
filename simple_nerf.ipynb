{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Synthesis\n",
    "\n",
    "> Implement by NeRF in pyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Description\n",
    "\n",
    "\"View synthesis\" is a task which\n",
    "generating images of a 3D scene from a specific point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Description\n",
    "\n",
    "\"NeRF\" (Neural Radiance Field) solved \"View synthesis\"\n",
    "by representing 3D scene using a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description\n",
    "\n",
    "1. Preprocessing\n",
    "2. Inference\n",
    "3. Rendering\n",
    "4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Preprocessing\n",
    "\n",
    "{{ True image }} → {{ Position, Direction, True color }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Inference\n",
    "\n",
    "{{ Position, Direction }} → {{ Network }} → {{ Color, Density }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Rendering\n",
    "\n",
    "{{ Color, Density }} → {{ Ray sampling }} → {{ Rendered color }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Training\n",
    "\n",
    "{{ True color, Rendered color, Network }} → {{ Network }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Description\n",
    "\n",
    "1. Positional Encoding of input coordinates\n",
    "    - For learning high-frequency features\n",
    "    - Using Fourier features\n",
    "2. Hierarchical Sampling\n",
    "    - For high-frequency representions\n",
    "    - Using two networks with different sample size\n",
    "3. Gradient Descent\n",
    "    - For minimizing the error between the true and rendered images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Details\n",
    "\n",
    "$$\n",
    "Predict(Position, Direction) = \\{Color, Density\\} \\\\\n",
    "\\text{where }\n",
    "\\begin{cases}\n",
    "    Predict \\text{ is MLP}, \\\\\n",
    "    Position \\in \\mathbb{R}^{3}, \\\\\n",
    "    Direction \\in \\mathbb{R}^{3}, \\\\\n",
    "    Color \\in \\mathbb{R}^{3}, \\\\\n",
    "    Density \\in \\mathbb{R}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "The raw and encoded coordinate values will be concatenated to form the network input.\n",
    "\n",
    "Each coordinate value in `Position` and `Direction` is encoded as follows:\n",
    "\n",
    "$$\n",
    "Encode_{N}(p) \\\\\n",
    "= \\{\\sin (2^0 \\pi p), \\cos (2^0 \\pi p), \\ldots, \\sin (2^{N-1} \\pi p), \\cos (2^{N-1} \\pi p)\\} \\\\\n",
    "= \\{\\sin (2^0 \\pi p), \\sin (\\frac{\\pi}{2} + 2^0 \\pi p), \\ldots, \\sin (2^{N-1} \\pi p), \\sin (\\frac{\\pi}{2} + 2^{N-1} \\pi p)\\} \\\\\n",
    "\\text{where } p \\in \\mathbb{R}, \\ N \\in \\mathbb{N}, \\ Encode_{N}(p) \\in \\mathbb{R}^{2N}\n",
    "$$\n",
    "\n",
    "The encoded dimensions are calculated as follows:\n",
    "\n",
    "| Input     | Dimension | N   | Encoded Dimension |\n",
    "| --------- | --------- | --- | ----------------- |\n",
    "| Position  | 3         | 10  | $3 (1 + 2N) = 63$ |\n",
    "| Direction | 3         | 4   | $3 (1 + 2N) = 27$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Details (Cont.)\n",
    "\n",
    "### Network\n",
    "\n",
    "The network is a multi-layer perceptron (MLP) with the following architecture:\n",
    "\n",
    "| Layer                                  | Input Dimension | Output Dimension | Activation |\n",
    "| -------------------------------------- | --------------- | ---------------- | ---------- |\n",
    "| Input Position                         | 63              | 256              | ReLU       |\n",
    "| Hidden 1                               | 256             | 256              | ReLU       |\n",
    "| Hidden 2                               | 256             | 256              | ReLU       |\n",
    "| Hidden 3                               | 256             | 256              | ReLU       |\n",
    "| Hidden 4                               | 256             | 256              | ReLU       |\n",
    "| Hidden 5 + Input Position (Skip conn.) | 256 + 63        | 256              | ReLU       |\n",
    "| Hidden 6                               | 256             | 256              | ReLU       |\n",
    "| Hidden 7                               | 256             | 256              | ReLU       |\n",
    "| Hidden 8 + Input Density               | 256 + 27        | 128              | ReLU       |\n",
    "| Output Density (from Hidden 7\\*)       | 256             | 1                | Custom     |\n",
    "| Output Color (from Hidden 8)           | 128             | 3                | Sigmoid    |\n",
    "\n",
    "<small>\\* The density is not dependent on the direction.</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Details\n",
    "\n",
    "Loss = Sum of (coarse - true)^2 + (fine - true)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. View synthesis. (n.d.). In Wikipedia. Retrieved from https://en.wikipedia.org/wiki/View_synthesis\n",
    "2. Neural radiance field. (n.d.). In Wikipedia. Retrieved from https://en.wikipedia.org/wiki/Neural_radiance_field\n",
    "3. Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., & Ng, R. (2020). NeRF: Neural radiance fields for image synthesis. arXiv preprint arXiv:2003.08934. Retrieved from https://arxiv.org/pdf/2003.08934\n",
    "4. Tancik, M., Srinivasan, P. P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J. T., & Ng, R. (2020). Fourier features let networks learn high frequency functions in low dimensional domains. NeurIPS. Retrieved from https://arxiv.org/pdf/2006.10739"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.types import Device\n",
    "\n",
    "\n",
    "class PositionalEncoder(Module):\n",
    "    def __init__(self, encoding_factor: int, device: Device | None = None):\n",
    "        import torch\n",
    "\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "\n",
    "        encoding_factor = max(int(encoding_factor), 0)\n",
    "\n",
    "        freq_lvls = torch.arange(encoding_factor, device=device)\n",
    "        self.freq = ((1 << freq_lvls) * torch.pi).repeat_interleave(2).reshape(-1, 1, 1)\n",
    "        sine_offsets = torch.tensor([[[0.0]], [[torch.pi / 2]]])\n",
    "        self.offsets = sine_offsets.repeat(encoding_factor, 1, 1)\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        feature_dim = self.freq.shape[0] * inputs.shape[-1]\n",
    "        features = (self.freq * inputs + self.offsets).sin_()\n",
    "        features = features.swapdims_(0, 1).reshape(*inputs.shape[:-1], feature_dim)\n",
    "        encoding = torch.concat([inputs, features], dim=-1)\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "class NeRF(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_count: int | None = None,\n",
    "        hidden_dim: int | None = None,\n",
    "        extra_hidden_dim: int | None = None,\n",
    "        position_encoding_factor: int | None = None,\n",
    "        direction_encoding_factor: int | None = None,\n",
    "    ):\n",
    "        from torch import nn\n",
    "\n",
    "        super(NeRF, self).__init__()\n",
    "\n",
    "        layer_count = int(layer_count or 8)\n",
    "        hidden_dim = int(hidden_dim or 256)\n",
    "        extra_hidden_dim = int(extra_hidden_dim or hidden_dim // 2)\n",
    "        position_encoding_factor = int(position_encoding_factor or 10)\n",
    "        direction_encoding_factor = int(direction_encoding_factor or 4)\n",
    "\n",
    "        COLOR_DIM = 3\n",
    "        DENSITY_DIM = 1\n",
    "        RAW_POSITION_DIM = 3\n",
    "        RAW_DIRECTION_DIM = 3\n",
    "        encoded_position_dim = RAW_POSITION_DIM * (1 + 2 * position_encoding_factor)\n",
    "        encoded_direction_dim = RAW_DIRECTION_DIM * (1 + 2 * direction_encoding_factor)\n",
    "\n",
    "        self.position_hidden_layer_skip_indexs = set(\n",
    "            [i for i in range(1, layer_count - 1) if i % 4 == 0]\n",
    "        )\n",
    "        self.position_input_layer = nn.Linear(encoded_position_dim, hidden_dim)\n",
    "        self.position_hidden_layers = nn.ModuleList(\n",
    "            [\n",
    "                (\n",
    "                    nn.Linear(hidden_dim + encoded_position_dim, hidden_dim)\n",
    "                    if i in self.position_hidden_layer_skip_indexs\n",
    "                    else nn.Linear(hidden_dim, hidden_dim)\n",
    "                )\n",
    "                for i in range(layer_count - 1)\n",
    "            ]\n",
    "        )\n",
    "        self.direction_input_layer = nn.Linear(\n",
    "            hidden_dim + encoded_direction_dim, extra_hidden_dim\n",
    "        )\n",
    "        self.density_output_layer = nn.Linear(hidden_dim, DENSITY_DIM)\n",
    "        self.color_output_layer = nn.Linear(extra_hidden_dim, COLOR_DIM)\n",
    "\n",
    "        self.position_input_encoder = PositionalEncoder(position_encoding_factor)\n",
    "        self.direction_input_encoder = PositionalEncoder(direction_encoding_factor)\n",
    "\n",
    "    def forward(self, inputs: Tensor):\n",
    "        import torch\n",
    "\n",
    "        raw_positions = inputs[:, :3]\n",
    "        raw_directions = inputs[:, 3:]\n",
    "        encoded_positions: Tensor = self.position_input_encoder(raw_positions)\n",
    "        encoded_directions: Tensor = self.direction_input_encoder(raw_directions)\n",
    "\n",
    "        hidden_position: Tensor = self.position_input_layer(encoded_positions)\n",
    "        for i, layer in enumerate(self.position_hidden_layers):\n",
    "            hidden_position.relu_()\n",
    "            hidden_position = layer(\n",
    "                torch.concat([hidden_position, encoded_positions], dim=-1)\n",
    "                if i in self.position_hidden_layer_skip_indexs\n",
    "                else hidden_position\n",
    "            )\n",
    "        hidden_position.relu_()\n",
    "\n",
    "        hidden_direction: Tensor = self.direction_input_layer(\n",
    "            torch.concat([hidden_position, encoded_directions], dim=-1)\n",
    "        ).relu_()\n",
    "        color: Tensor = self.color_output_layer(hidden_direction).sigmoid_()\n",
    "        density: Tensor = self.density_output_layer(hidden_position)\n",
    "\n",
    "        return density, color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeRF(\n",
       "  (position_input_layer): Linear(in_features=63, out_features=256, bias=True)\n",
       "  (position_hidden_layers): ModuleList(\n",
       "    (0-3): 4 x Linear(in_features=256, out_features=256, bias=True)\n",
       "    (4): Linear(in_features=319, out_features=256, bias=True)\n",
       "    (5-6): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (direction_input_layer): Linear(in_features=283, out_features=128, bias=True)\n",
       "  (density_output_layer): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (color_output_layer): Linear(in_features=128, out_features=3, bias=True)\n",
       "  (position_input_encoder): PositionalEncoder()\n",
       "  (direction_input_encoder): PositionalEncoder()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NeRF()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
