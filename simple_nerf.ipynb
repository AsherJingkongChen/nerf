{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @page {\n",
       "        size: A3 landscape;\n",
       "        margin: 0;\n",
       "    }\n",
       "    .jp-Cell:first-child {\n",
       "        display: none;\n",
       "    }\n",
       "    .jp-RenderedMermaid {\n",
       "        justify-content: center;\n",
       "    }\n",
       "    h2 {\n",
       "        page-break-before: always;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    @page {\n",
    "        size: A3 landscape;\n",
    "        margin: 0;\n",
    "    }\n",
    "    .jp-Cell:first-child {\n",
    "        display: none;\n",
    "    }\n",
    "    .jp-RenderedMermaid {\n",
    "        justify-content: center;\n",
    "    }\n",
    "    h2 {\n",
    "        page-break-before: always;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Synthesis\n",
    "\n",
    "> Implement by NeRF in pyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Description\n",
    "\n",
    "\"View synthesis\" is a task which\n",
    "generating images of a 3D scene from a specific point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Description\n",
    "\n",
    "\"NeRF\" (Neural Radiance Field) solved \"View synthesis\"\n",
    "by representing 3D scene using a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description\n",
    "\n",
    "1. Preprocessing\n",
    "2. Inference\n",
    "3. Rendering\n",
    "4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Preprocessing\n",
    "\n",
    "{{ True image }} → {{ Position, Direction, True color }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Inference\n",
    "\n",
    "{{ Position, Direction }} → {{ Volumetric sampling }} → {{ Positional encoding }} → {{ Network }} → {{ Color, Density }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Rendering\n",
    "\n",
    "{{ Color, Density }} → {{ Alpha blending }} → {{ Rendered color }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Training\n",
    "\n",
    "{{ True color, Rendered color, Network }} → {{ Network }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    subgraph Preprocessing\n",
    "        ti[True Image]\n",
    "        cp[Camera Posture]\n",
    "        tc[True Color]\n",
    "    end\n",
    "\n",
    "    subgraph Inference\n",
    "        vs[Volume Sampling]\n",
    "        B --> C[Ray Generation]\n",
    "        C --> D[Ray Batching]\n",
    "        D --> E[Volume Sampling]\n",
    "        E --> F[Positional Encoding]\n",
    "        F --> G[Network]\n",
    "        G --> H[Color, Density]\n",
    "    end\n",
    "\n",
    "    subgraph Rendering\n",
    "        H --> I[Alpha Blending]\n",
    "        I --> J[Rendered Color]\n",
    "    end\n",
    "\n",
    "    subgraph Training\n",
    "        B -.-> L[Loss Calculation]\n",
    "        J -.-> L\n",
    "        G -.-> L\n",
    "        L --> G\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Description\n",
    "\n",
    "1. Positional Encoding of input coordinates\n",
    "    - For learning high-frequency features\n",
    "    - Using Fourier features\n",
    "2. Stochastic Gradient Descent\n",
    "    - For minimizing the error between the true and rendered images\n",
    "    - Choosing a random image from the dataset each iteration\n",
    "<!-- 3. Hierarchical Sampling\n",
    "    - For high-frequency representions\n",
    "    - Using two networks with different sample size -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Details\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "The raw and encoded coordinate values will be concatenated to form the network input.\n",
    "\n",
    "Each coordinate value in `Position` and `Direction` is encoded as follows:\n",
    "\n",
    "$$\n",
    "Encode_{N}(p) \\\\\n",
    "\n",
    "= \\{\\sin (2^0 \\pi p), \\cos (2^0 \\pi p), \\ldots, \\sin (2^{N-1} \\pi p), \\cos (2^{N-1} \\pi p)\\} \\\\\n",
    "\n",
    "= \\{\\sin (2^0 \\pi p), \\sin (\\frac{\\pi}{2} + 2^0 \\pi p), \\ldots, \\sin (2^{N-1} \\pi p), \\sin (\\frac{\\pi}{2} + 2^{N-1} \\pi p)\\} \\\\\n",
    "\n",
    "\\text{where } p \\in \\mathbb{R}, \\ N \\in \\mathbb{N}, \\ Encode_{N}(p) \\in \\mathbb{R}^{2N}\n",
    "$$\n",
    "\n",
    "The encoded dimensions are calculated as follows:\n",
    "\n",
    "| Input     | Dimension | N   | Encoded Dimension |\n",
    "| --------- | --------- | --- | ----------------- |\n",
    "| Position  | 3         | 10  | $3 (1 + 2N) = 63$ |\n",
    "| Direction | 3         | 4   | $3 (1 + 2N) = 27$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Details (Cont.)\n",
    "\n",
    "### Network Definition\n",
    "\n",
    "The neural network is a multi-layer perceptron (MLP) with the following structure:\n",
    "- The density is not dependent on the direction\n",
    "- The fifth hidden layer concatenates the input as a skip connection\n",
    "\n",
    "```mermaid\n",
    "%%{init: {\n",
    "    \"theme\": \"neutral\",\n",
    "    \"themeVariables\": {\n",
    "        \"fontFamily\": \"Menlo, monospace\",\n",
    "        \"fontSize\": \"10px\"\n",
    "    }\n",
    "}}%%\n",
    "flowchart TD\n",
    "    ip1([Input Position 3])\n",
    "    ep1([Encoded Position 63])\n",
    "    h1([Hidden Layer 256])\n",
    "    h2([Hidden Layer 256])\n",
    "    h3([Hidden Layer 256])\n",
    "    h4([Hidden Layer 256])\n",
    "    h5([Hidden Layer 256])\n",
    "    ip2([Input Position 3])\n",
    "    ep2([Encoded Position 63])\n",
    "    h6([Hidden Layer 256])\n",
    "    h7([Hidden Layer 256])\n",
    "    h8([Hidden Layer 256])\n",
    "    od([Output Density 1])\n",
    "    iof([Input/Output Feature 256])\n",
    "    id([Input Direction 3])\n",
    "    ed([Encoded Direction 27])\n",
    "    ha([Additional Hidden Layer 128])\n",
    "    oc([Output Color 3])\n",
    "\n",
    "    ip1 -->|Encode| ep1\n",
    "    ep1 -->|ReLU| h1\n",
    "    h1 -->|ReLU| h2\n",
    "    h2 -->|ReLU| h3\n",
    "    h3 -->|ReLU| h4\n",
    "    h4 -->|ReLU| h5\n",
    "    ip2 -->|Encode| ep2\n",
    "    ep2 ---|Concatenate| h5\n",
    "    h5 -->|ReLU| h6\n",
    "    h6 -->|ReLU| h7\n",
    "    h7 -->|ReLU| h8\n",
    "    h8 -->|ReLU| od\n",
    "    h8 --> iof\n",
    "    id -->|Encode| ed\n",
    "    ed ---|Concatenate| iof\n",
    "    iof -->|ReLU| ha\n",
    "    ha -->|Sigmoid| oc\n",
    "\n",
    "    style ip1 fill:palegreen\n",
    "    style ip2 fill:palegreen\n",
    "    style id fill:palegreen\n",
    "    style ep2 fill:mediumaquamarine\n",
    "    style ep1 fill:mediumaquamarine\n",
    "    style ed fill:mediumaquamarine\n",
    "    style h1 fill:deepskyblue\n",
    "    style h2 fill:deepskyblue\n",
    "    style h3 fill:deepskyblue\n",
    "    style h4 fill:deepskyblue\n",
    "    style h5 fill:deepskyblue\n",
    "    style h6 fill:deepskyblue\n",
    "    style h7 fill:deepskyblue\n",
    "    style h8 fill:deepskyblue\n",
    "    style ha fill:deepskyblue\n",
    "    style iof fill:tan\n",
    "    style od fill:salmon\n",
    "    style oc fill:salmon\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Details (Cont.)\n",
    "\n",
    "### Volume Sampling\n",
    "\n",
    "To represent a continuous scene, we can sample points along the rays, which can be written as $r(t_{i}) = o + d t_{i}$ where:\n",
    "- $o$ is the origin point\n",
    "- $d$ is the direction vector\n",
    "- $t_{i} \\sim U[\\frac{i - 1}{N}, \\frac{i}{N}]$ is the distance along the ray (Stratified sampling)\n",
    "- $N$ is the number of samples per ray, we use $N = 96$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering Details (Cont.)\n",
    "\n",
    "### Volume Rendering\n",
    "\n",
    "$$\n",
    "\\hat{C} = \\sum_{i=1}^{N} T_{i} \\alpha_{i} c_{i} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Details\n",
    "\n",
    "| Module Name                       | Details                                    |\n",
    "| --------------------------------- | ------------------------------------------ |\n",
    "| Optimizer - Adam                  | Learning rate is $5 \\times 10^{-4}$        |\n",
    "| Loss function - Mean Square Error | Error between the true and rendered colors |\n",
    "| Data loader                       | 1024 rays per batch to reduce memory cost  |\n",
    "\n",
    "$$\n",
    "Loss = \\frac{\\Sigma_{r \\in \\mathbb{R}} (C_{rendered}(r) - C_{true}(r))^2}{|\\mathbb{R}|} \\\\\n",
    "\n",
    "\\text{where } \\mathbb{R} \\text{ is a batch of rays}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. View synthesis. (n.d.). In Wikipedia. Retrieved from https://en.wikipedia.org/wiki/View_synthesis\n",
    "2. Neural radiance field. (n.d.). In Wikipedia. Retrieved from https://en.wikipedia.org/wiki/Neural_radiance_field\n",
    "3. Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., & Ng, R. (2020). NeRF: Neural radiance fields for image synthesis. arXiv preprint arXiv:2003.08934. Retrieved from https://arxiv.org/pdf/2003.08934\n",
    "4. Tancik, M., Srinivasan, P. P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J. T., & Ng, R. (2020). Fourier features let networks learn high frequency functions in low dimensional domains. NeurIPS. Retrieved from https://arxiv.org/pdf/2006.10739"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.types import Device\n",
    "\n",
    "\n",
    "class PositionalEncoder(Module):\n",
    "    \"\"\"\n",
    "    ## Arguments\n",
    "    - `encoding_factor`: `int`\n",
    "\n",
    "    ## Inputs\n",
    "    1. `Vectors`: `[..., dim]`\n",
    "\n",
    "    ## Outputs\n",
    "    1. `Features`: `[..., dim * (2 * encoding_factor + 1)]`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding_factor: int, device: Device | None = None):\n",
    "        import torch\n",
    "\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "\n",
    "        encoding_factor = max(int(encoding_factor), 0)\n",
    "\n",
    "        freq_lvls = torch.arange(encoding_factor, device=device)\n",
    "        self.freq = ((1 << freq_lvls) * torch.pi).repeat_interleave(2)\n",
    "        sine_offsets = torch.tensor([0.0, torch.pi / 2])\n",
    "        self.offsets = sine_offsets.repeat(encoding_factor)\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        inputs = torch.as_tensor(inputs).unsqueeze(-1)\n",
    "\n",
    "        features = (self.freq * inputs + self.offsets).sin_()\n",
    "        features = torch.concat([inputs, features], dim=-1)\n",
    "        features = features.reshape(*inputs.shape[:-2], -1)\n",
    "        return features\n",
    "\n",
    "    def get_last_dim(self, input_dim: int) -> int:\n",
    "        return int(input_dim) * (self.freq.shape[0] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "class MLP_NeRF(Module):\n",
    "    \"\"\"\n",
    "    ## Inputs\n",
    "    1. `positions`: `[..., 3]`\n",
    "    2. `directions`: `[..., 3]`\n",
    "\n",
    "    ## Outputs\n",
    "    1. `colors`: `[..., 3]`\n",
    "    2. `densities`: `[..., 1]`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_count: int | None = None,\n",
    "        hidden_dim: int | None = None,\n",
    "        additional_hidden_dim: int | None = None,\n",
    "        position_encoder: PositionalEncoder | None = None,\n",
    "        direction_encoder: PositionalEncoder | None = None,\n",
    "    ):\n",
    "        from torch import nn\n",
    "\n",
    "        super(MLP_NeRF, self).__init__()\n",
    "\n",
    "        layer_count = int(layer_count or 8)\n",
    "        hidden_dim = int(hidden_dim or 256)\n",
    "        additional_hidden_dim = int(additional_hidden_dim or hidden_dim // 2)\n",
    "        if position_encoder is None:\n",
    "            position_encoder = PositionalEncoder(10)\n",
    "        if direction_encoder is None:\n",
    "            direction_encoder = PositionalEncoder(4)\n",
    "\n",
    "        COLOR_DIM = 3\n",
    "        DENSITY_DIM = 1\n",
    "        POSITION_DIM = 3\n",
    "        DIRECTION_DIM = 3\n",
    "        encoded_position_dim = position_encoder.get_last_dim(POSITION_DIM)\n",
    "        encoded_direction_dim = direction_encoder.get_last_dim(DIRECTION_DIM)\n",
    "\n",
    "        self.position_hidden_layer_skip_indexs = set(\n",
    "            [i for i in range(1, layer_count - 1) if i % 4 == 0]\n",
    "        )\n",
    "        self.position_input_layer = nn.Linear(encoded_position_dim, hidden_dim)\n",
    "        self.position_hidden_layers = nn.ModuleList(\n",
    "            [\n",
    "                (\n",
    "                    nn.Linear(hidden_dim + encoded_position_dim, hidden_dim)\n",
    "                    if i in self.position_hidden_layer_skip_indexs\n",
    "                    else nn.Linear(hidden_dim, hidden_dim)\n",
    "                )\n",
    "                for i in range(layer_count)\n",
    "            ]\n",
    "        )\n",
    "        self.density_output_layer = nn.Linear(hidden_dim, DENSITY_DIM)\n",
    "        self.direction_input_layer = nn.Linear(\n",
    "            hidden_dim + encoded_direction_dim,\n",
    "            additional_hidden_dim,\n",
    "        )\n",
    "        self.color_output_layer = nn.Linear(additional_hidden_dim, COLOR_DIM)\n",
    "\n",
    "        self.position_input_encoder = position_encoder\n",
    "        self.direction_input_encoder = direction_encoder\n",
    "\n",
    "    def forward(self, positions: Tensor, directions: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        import torch\n",
    "\n",
    "        positions = torch.as_tensor(positions)\n",
    "        directions = torch.as_tensor(directions)\n",
    "\n",
    "        encoded_positions: Tensor = self.position_input_encoder(positions)\n",
    "        encoded_directions: Tensor = self.direction_input_encoder(directions)\n",
    "\n",
    "        hidden_positions: Tensor = self.position_input_layer(encoded_positions)\n",
    "        for index, layer in enumerate(self.position_hidden_layers):\n",
    "            hidden_positions.relu_()\n",
    "            hidden_positions = layer(\n",
    "                torch.concat([hidden_positions, encoded_positions], dim=-1)\n",
    "                if index in self.position_hidden_layer_skip_indexs\n",
    "                else hidden_positions\n",
    "            )\n",
    "\n",
    "        density: Tensor = self.density_output_layer(hidden_positions).relu_()\n",
    "        hidden_directions: Tensor = self.direction_input_layer(\n",
    "            torch.concat([hidden_positions, encoded_directions], dim=-1)\n",
    "        ).relu_()\n",
    "        color: Tensor = self.color_output_layer(hidden_directions).sigmoid_()\n",
    "\n",
    "        return color, density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogNormalInitializer:\n",
    "    \"\"\"\n",
    "    ## Examples\n",
    "    ```python\n",
    "    from torch.nn import Module\n",
    "\n",
    "    Module().apply(LogNormalInitializer(mean=0.0, std=2.0, seed=1))\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mean: float | None = None,\n",
    "        std: float | None = None,\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        from torch import Generator\n",
    "\n",
    "        mean = float(mean or 0.0)\n",
    "        std = float(std or 2.0)\n",
    "        if seed is not None:\n",
    "            seed = int(seed)\n",
    "\n",
    "        self.generator: Generator = seed\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, module: Module) -> None:\n",
    "        import torch\n",
    "\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            if module.weight is not None:\n",
    "                with torch.no_grad():\n",
    "                    if type(self.generator) is int:\n",
    "                        self.generator = torch.Generator(\n",
    "                            module.weight.device,\n",
    "                        ).manual_seed(\n",
    "                            self.generator,\n",
    "                        )\n",
    "                    epsilon = torch.finfo(torch.float).eps\n",
    "                    module.weight.log_normal_(\n",
    "                        mean=self.mean,\n",
    "                        std=self.std,\n",
    "                        generator=self.generator,\n",
    "                    ).clamp_min_(\n",
    "                        epsilon,\n",
    "                    )\n",
    "                    module.weight.div_(\n",
    "                        module.weight.max(),\n",
    "                    ).clamp_min_(\n",
    "                        epsilon,\n",
    "                    )\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Ray Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.types import Device\n",
    "\n",
    "\n",
    "class RayGenerator:\n",
    "    \"\"\"\n",
    "    ## Arguments\n",
    "    - `height`: `int`\n",
    "    - `width`: `int`\n",
    "    - `focal`: `float`\n",
    "\n",
    "    ## Inputs\n",
    "    1. `posture`: `[4, 4]`\n",
    "\n",
    "    ## Outputs\n",
    "    1. `origins`: `[height, width, 3]`\n",
    "    2. `directions`: `[height, width, 3]`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        height: int,\n",
    "        width: int,\n",
    "        focal: float,\n",
    "        device: Device | None = None,\n",
    "    ):\n",
    "        import torch\n",
    "\n",
    "        focal_inverse = 1.0 / float(focal)\n",
    "        unit_half_norm = focal_inverse / 2\n",
    "        height_half_norm = height * unit_half_norm\n",
    "        width_half_norm = width * unit_half_norm\n",
    "\n",
    "        self.directions = torch.stack(\n",
    "            torch.meshgrid(\n",
    "                torch.arange(\n",
    "                    -width_half_norm + unit_half_norm,\n",
    "                    width_half_norm,\n",
    "                    focal_inverse,\n",
    "                    device=device,\n",
    "                ),\n",
    "                torch.arange(\n",
    "                    height_half_norm - unit_half_norm,\n",
    "                    -height_half_norm,\n",
    "                    -focal_inverse,\n",
    "                    device=device,\n",
    "                ),\n",
    "                torch.tensor(-1.0),\n",
    "                indexing=\"xy\",\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "    def __call__(self, posture: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        posture = torch.as_tensor(posture)[:3]\n",
    "\n",
    "        directions = (self.directions * posture[:, :3]).sum(dim=-1)\n",
    "        directions.div_(directions.norm(dim=-1, keepdim=True))\n",
    "        origins = posture[:, 3].broadcast_to(directions.shape)\n",
    "        return origins, directions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Stratified Point Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StratifiedPointSampler:\n",
    "    \"\"\"\n",
    "    ## Arguments\n",
    "    - `points_per_ray`: `int`\n",
    "\n",
    "    ## Inputs\n",
    "    1. `origins`: `[..., 3]`\n",
    "    2. `directions`: `[..., 3]`\n",
    "\n",
    "    ## Outputs\n",
    "    1. `points`: `[..., points_per_ray, 3]`\n",
    "    2. `intervals`: `[points_per_ray]` (Ended with `torch.finfo(torch.float).max`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, points_per_ray: int | None = None, seed: int | None = None):\n",
    "        from torch import Generator\n",
    "\n",
    "        points_per_ray = int(points_per_ray or 96)\n",
    "        if seed is not None:\n",
    "            seed = int(seed)\n",
    "\n",
    "        self.generator: Generator = seed\n",
    "        self.points_per_ray = points_per_ray\n",
    "\n",
    "    def __call__(self, origins: Tensor, directions: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        origins = torch.as_tensor(origins).unsqueeze(-2)\n",
    "        directions = torch.as_tensor(directions).unsqueeze(-2)\n",
    "\n",
    "        device = origins.device\n",
    "        if type(self.generator) is int:\n",
    "            self.generator = torch.Generator(device).manual_seed(self.generator)\n",
    "\n",
    "        distances = (\n",
    "            torch.arange(\n",
    "                self.points_per_ray,\n",
    "                device=device,\n",
    "                dtype=torch.float,\n",
    "            )\n",
    "            .add_(\n",
    "                torch.rand(\n",
    "                    self.points_per_ray,\n",
    "                    device=device,\n",
    "                    generator=self.generator,\n",
    "                )\n",
    "            )\n",
    "            .div_(self.points_per_ray)\n",
    "        )\n",
    "        points = origins + directions * distances.unsqueeze(-1)\n",
    "        intervals = distances[..., 1:] - distances[..., :-1]\n",
    "        intervals = torch.concat(\n",
    "            [intervals, torch.tensor([torch.finfo(torch.float).max])], dim=-1\n",
    "        )\n",
    "        return points, intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((106, 100, 100, 3), (106, 4, 4), array(138.8888789))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load('tiny_nerf_data.npz')\n",
    "images = data['images']\n",
    "poses = data['poses']\n",
    "focal = data['focal']\n",
    "H, W = images.shape[1:3]\n",
    "(images.shape, poses.shape, focal)\n",
    "# H W C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataLoader:\n",
    "    def __init__(self, url: str, seed: int | None = None) -> None:\n",
    "        from httpx import get\n",
    "        from io import BytesIO\n",
    "        from numpy import load\n",
    "        from random import seed as set_seed\n",
    "        import torch\n",
    "\n",
    "        with BytesIO(\n",
    "            get(url, follow_redirects=True).raise_for_status().read()\n",
    "        ) as numpy_file:\n",
    "            arrays = load(numpy_file)\n",
    "            self.focal = torch.as_tensor(arrays[\"focal\"], dtype=torch.float)\n",
    "            self.images = torch.as_tensor(arrays[\"images\"])\n",
    "            self.postures = torch.as_tensor(arrays[\"poses\"])\n",
    "\n",
    "            if self.images.shape[0] != self.postures.shape[0]:\n",
    "                raise ValueError(\"The number of images and postures must be the same.\")\n",
    "\n",
    "            self.size = self.images.shape[0]\n",
    "\n",
    "        if seed is not None:\n",
    "            seed = int(seed)\n",
    "\n",
    "        set_seed(seed)\n",
    "\n",
    "    def get_image_and_posture(self, index: int | None = None) -> tuple[Tensor, Tensor]:\n",
    "        from random import randint\n",
    "\n",
    "        if index is not None:\n",
    "            index = int(index)\n",
    "        else:\n",
    "            index = randint(0, self.size - 1)\n",
    "\n",
    "        return self.images[index], self.postures[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\"http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=100x100>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "Image.fromarray((loader.get_image_and_posture()[0] * 255).type(torch.uint8).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 100, 3]),\n",
       " torch.Size([100, 100, 3]),\n",
       " torch.Size([100, 100, 96, 3]),\n",
       " torch.Size([96]))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raygen = RayGenerator(height=H, width=W, focal=focal)\n",
    "raysampler = StratifiedPointSampler(96)\n",
    "origins, directions = raygen(posture=poses[0])\n",
    "points, intervals = raysampler(origins, directions)\n",
    "origins.shape, directions.shape, points.shape, intervals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 3])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directions.unsqueeze(-2).broadcast_to(points.shape)[0, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02,\n",
       "        1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02,\n",
       "        1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02,\n",
       "        1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02,\n",
       "        1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02,\n",
       "        1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02,\n",
       "        1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02,\n",
       "        1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02,\n",
       "        1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02,\n",
       "        1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02, 1.5873e-02,\n",
       "        1.5873e-02, 1.5873e-02, 1.5873e-02, 3.4028e+38])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as tf\n",
    "z_vals = tf.linspace(0, 1, 64)\n",
    "intv = tf.concat([z_vals[..., 1:] - z_vals[..., :-1], tf.tensor([torch.finfo(torch.float).max])], -1)\n",
    "# alpha = 1.-tf.exp(-sigma_a * dists)  \n",
    "# weights = alpha * tf.math.cumprod(1.-alpha + 1e-10, -1, exclusive=True)\n",
    "intv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
