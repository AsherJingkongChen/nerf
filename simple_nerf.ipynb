{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @page {\n",
       "        size: A3 landscape;\n",
       "        margin: 0;\n",
       "    }\n",
       "    .jp-Cell:first-child {\n",
       "        display: none;\n",
       "    }\n",
       "    .jp-RenderedMermaid {\n",
       "        justify-content: center;\n",
       "    }\n",
       "    h2 {\n",
       "        page-break-before: always;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    @page {\n",
    "        size: A3 landscape;\n",
    "        margin: 0;\n",
    "    }\n",
    "    .jp-Cell:first-child {\n",
    "        display: none;\n",
    "    }\n",
    "    .jp-RenderedMermaid {\n",
    "        justify-content: center;\n",
    "    }\n",
    "    h2 {\n",
    "        page-break-before: always;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Synthesis\n",
    "\n",
    "> Implement by NeRF in pyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Description\n",
    "\n",
    "\"View synthesis\" is a task which\n",
    "generating images of a 3D scene from a specific point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Description\n",
    "\n",
    "\"NeRF\" (Neural Radiance Field) solved \"View synthesis\"\n",
    "by representing 3D scene using a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description\n",
    "\n",
    "1. Preprocessing\n",
    "2. Inference\n",
    "3. Rendering\n",
    "4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Preprocessing\n",
    "\n",
    "{{ True image }} → {{ Position, Direction, True color }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Inference\n",
    "\n",
    "{{ Position, Direction }} → {{ Volumetric sampling }} → {{ Positional encoding }} → {{ Network }} → {{ Color, Density }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Rendering\n",
    "\n",
    "{{ Color, Density }} → {{ Alpha blending }} → {{ Rendered color }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Description - Training\n",
    "\n",
    "{{ True color, Rendered color, Network }} → {{ Network }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    subgraph Preprocessing\n",
    "        ti[True Image]\n",
    "        cp[Camera Posture]\n",
    "        tc[True Color]\n",
    "    end\n",
    "\n",
    "    subgraph Inference\n",
    "        vs[Volume Sampling]\n",
    "        B --> C[Ray Generation]\n",
    "        C --> D[Ray Batching]\n",
    "        D --> E[Volume Sampling]\n",
    "        E --> F[Positional Encoding]\n",
    "        F --> G[Network]\n",
    "        G --> H[Color, Density]\n",
    "    end\n",
    "\n",
    "    subgraph Rendering\n",
    "        H --> I[Alpha Blending]\n",
    "        I --> J[Rendered Color]\n",
    "    end\n",
    "\n",
    "    subgraph Training\n",
    "        B -.-> L[Loss Calculation]\n",
    "        J -.-> L\n",
    "        G -.-> L\n",
    "        L --> G\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Description\n",
    "\n",
    "1. Positional Encoding of input coordinates\n",
    "    - For learning high-frequency features\n",
    "    - Using Fourier features\n",
    "2. Stochastic Gradient Descent\n",
    "    - For minimizing the error between the true and rendered images\n",
    "    - Choosing a random image from the dataset each iteration\n",
    "<!-- 3. Hierarchical Sampling\n",
    "    - For high-frequency representions\n",
    "    - Using two networks with different sample size -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Details\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "The raw and encoded coordinate values will be concatenated to form the network input.\n",
    "\n",
    "Each coordinate value in `Position` and `Direction` is encoded as follows:\n",
    "\n",
    "$$\n",
    "Encode_{N}(p) \\\\\n",
    "\n",
    "= \\{\\sin (2^0 \\pi p), \\cos (2^0 \\pi p), \\ldots, \\sin (2^{N-1} \\pi p), \\cos (2^{N-1} \\pi p)\\} \\\\\n",
    "\n",
    "= \\{\\sin (2^0 \\pi p), \\sin (\\frac{\\pi}{2} + 2^0 \\pi p), \\ldots, \\sin (2^{N-1} \\pi p), \\sin (\\frac{\\pi}{2} + 2^{N-1} \\pi p)\\} \\\\\n",
    "\n",
    "\\text{where } p \\in \\mathbb{R}, \\ N \\in \\mathbb{N}, \\ Encode_{N}(p) \\in \\mathbb{R}^{2N}\n",
    "$$\n",
    "\n",
    "The encoded dimensions are calculated as follows:\n",
    "\n",
    "| Input     | Dimension | N   | Encoded Dimension |\n",
    "| --------- | --------- | --- | ----------------- |\n",
    "| Position  | 3         | 10  | $3 (1 + 2N) = 63$ |\n",
    "| Direction | 3         | 4   | $3 (1 + 2N) = 27$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Details (Cont.)\n",
    "\n",
    "### Neural Prediction\n",
    "\n",
    "The neural network is a multi-layer perceptron (MLP) with the following structure:\n",
    "- The density is not dependent on the direction\n",
    "- The fifth hidden layer concatenates the input as a skip connection\n",
    "\n",
    "```mermaid\n",
    "%%{init: {\n",
    "    \"theme\": \"neutral\",\n",
    "    \"themeVariables\": {\n",
    "        \"fontFamily\": \"Menlo, monospace\",\n",
    "        \"fontSize\": \"10px\"\n",
    "    }\n",
    "}}%%\n",
    "flowchart TD\n",
    "    ip1([Input Position 3])\n",
    "    ep1([Encoded Position 63])\n",
    "    h1([Hidden Layer 256])\n",
    "    h2([Hidden Layer 256])\n",
    "    h3([Hidden Layer 256])\n",
    "    h4([Hidden Layer 256])\n",
    "    h5([Hidden Layer 256])\n",
    "    ip2([Input Position 3])\n",
    "    ep2([Encoded Position 63])\n",
    "    h6([Hidden Layer 256])\n",
    "    h7([Hidden Layer 256])\n",
    "    h8([Hidden Layer 256])\n",
    "    od([Output Density 1])\n",
    "    iof([Input/Output Feature 256])\n",
    "    id([Input Direction 3])\n",
    "    ed([Encoded Direction 27])\n",
    "    ha([Additional Hidden Layer 128])\n",
    "    oc([Output Color 3])\n",
    "\n",
    "    ip1 -->|Encode| ep1\n",
    "    ep1 -->|ReLU| h1\n",
    "    h1 -->|ReLU| h2\n",
    "    h2 -->|ReLU| h3\n",
    "    h3 -->|ReLU| h4\n",
    "    h4 -->|ReLU| h5\n",
    "    ip2 -->|Encode| ep2\n",
    "    ep2 ---|Concatenate| h5\n",
    "    h5 -->|ReLU| h6\n",
    "    h6 -->|ReLU| h7\n",
    "    h7 -->|ReLU| h8\n",
    "    h8 -->|ReLU| od\n",
    "    h8 --> iof\n",
    "    id -->|Encode| ed\n",
    "    ed ---|Concatenate| iof\n",
    "    iof -->|ReLU| ha\n",
    "    ha -->|Sigmoid| oc\n",
    "\n",
    "    style ip1 fill:palegreen\n",
    "    style ip2 fill:palegreen\n",
    "    style id fill:palegreen\n",
    "    style ep2 fill:mediumaquamarine\n",
    "    style ep1 fill:mediumaquamarine\n",
    "    style ed fill:mediumaquamarine\n",
    "    style h1 fill:deepskyblue\n",
    "    style h2 fill:deepskyblue\n",
    "    style h3 fill:deepskyblue\n",
    "    style h4 fill:deepskyblue\n",
    "    style h5 fill:deepskyblue\n",
    "    style h6 fill:deepskyblue\n",
    "    style h7 fill:deepskyblue\n",
    "    style h8 fill:deepskyblue\n",
    "    style ha fill:deepskyblue\n",
    "    style iof fill:tan\n",
    "    style od fill:salmon\n",
    "    style oc fill:salmon\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Details (Cont.)\n",
    "\n",
    "### Stratified Point Sampling\n",
    "\n",
    "To represent a continuous scene, we can sample points along the rays, which can be written as $r(t_{i}) = o + d t_{i}$ where:\n",
    "- $o$ is the origin point\n",
    "- $d$ is the direction vector\n",
    "- $t_{i} \\sim U[\\frac{i - 1}{N}, \\frac{i}{N}]$ is the distance along the ray (Stratified sampling)\n",
    "- $N$ is the number of samples per ray, we use $N = 32$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering Details (Cont.)\n",
    "\n",
    "### Volume Rendering\n",
    "\n",
    "$$\n",
    "\\hat{C} = \\sum_{i=1}^{N} T_{i} \\alpha_{i} c_{i} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Details\n",
    "\n",
    "| Module Name                       | Details                                    |\n",
    "| --------------------------------- | ------------------------------------------ |\n",
    "| Weight updater - Adam             | Learning rate is $5 \\times 10^{-4}$        |\n",
    "| Loss function - Mean Square Error | Error between the true and rendered colors |\n",
    "| Data loader                       | 1024 rays per batch to reduce memory cost  |\n",
    "\n",
    "$$\n",
    "Loss = \\frac{\\Sigma_{r \\in \\mathbb{R}} (C_{rendered}(r) - C_{true}(r))^2}{|\\mathbb{R}|} \\\\\n",
    "\n",
    "\\text{where } \\mathbb{R} \\text{ is a batch of rays}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. View synthesis. (n.d.). In Wikipedia. Retrieved from https://en.wikipedia.org/wiki/View_synthesis\n",
    "2. Neural radiance field. (n.d.). In Wikipedia. Retrieved from https://en.wikipedia.org/wiki/Neural_radiance_field\n",
    "3. Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., & Ng, R. (2020). NeRF: Neural radiance fields for image synthesis. arXiv preprint arXiv:2003.08934. Retrieved from https://arxiv.org/pdf/2003.08934\n",
    "4. Tancik, M., Srinivasan, P. P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J. T., & Ng, R. (2020). Fourier features let networks learn high frequency functions in low dimensional domains. NeurIPS. Retrieved from https://arxiv.org/pdf/2006.10739"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.types import Device\n",
    "\n",
    "\n",
    "class PositionalEncoder(Module):\n",
    "    \"\"\"\n",
    "    ## Arguments\n",
    "    - `encoding_factor`: `int`\n",
    "\n",
    "    ## Inputs\n",
    "    1. `Vectors`: `[..., dim]`\n",
    "\n",
    "    ## Outputs\n",
    "    1. `Features`: `[..., dim * (2 * encoding_factor + 1)]`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding_factor: int, device: Device | None = None):\n",
    "        import torch\n",
    "\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "\n",
    "        encoding_factor = max(int(encoding_factor), 0)\n",
    "\n",
    "        freq_lvls = torch.arange(encoding_factor, device=device)\n",
    "        self.freq = ((2 ** freq_lvls) * torch.pi).repeat_interleave(2)\n",
    "        sine_offsets = torch.tensor([0.0, torch.pi / 2], device=device)\n",
    "        self.offsets = sine_offsets.repeat(encoding_factor)\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        inputs = torch.as_tensor(inputs).unsqueeze(-1)\n",
    "\n",
    "        features = (self.freq * inputs + self.offsets).sin()\n",
    "        features = torch.concat([inputs, features], dim=-1)\n",
    "        features = features.reshape(*inputs.shape[:-2], -1)\n",
    "        return features\n",
    "\n",
    "    def get_last_dim(self, input_dim: int) -> int:\n",
    "        return int(input_dim) * (self.freq.shape[0] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Neural Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.types import Device\n",
    "\n",
    "\n",
    "class MLP(Module):\n",
    "    \"\"\"\n",
    "    ## Inputs\n",
    "    1. `positions` + `directions`: `[..., 3 + 3]`\n",
    "\n",
    "    ## Outputs\n",
    "    1. `colors` + `densities`: `[..., 3 + 1]`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_count: int | None = None,\n",
    "        hidden_dim: int | None = None,\n",
    "        additional_hidden_dim: int | None = None,\n",
    "        position_encoder: PositionalEncoder | None = None,\n",
    "        direction_encoder: PositionalEncoder | None = None,\n",
    "        device: Device | None = None,\n",
    "    ):\n",
    "        from torch import nn\n",
    "\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        layer_count = int(layer_count or 8)\n",
    "        hidden_dim = int(hidden_dim or 256)\n",
    "        additional_hidden_dim = int(additional_hidden_dim or hidden_dim // 2)\n",
    "        if position_encoder is None:\n",
    "            position_encoder = PositionalEncoder(10, device=device)\n",
    "        if direction_encoder is None:\n",
    "            direction_encoder = PositionalEncoder(4, device=device)\n",
    "\n",
    "        COLOR_DIM = 3\n",
    "        DENSITY_DIM = 1\n",
    "        POSITION_DIM = 3\n",
    "        DIRECTION_DIM = 3\n",
    "        encoded_position_dim = position_encoder.get_last_dim(POSITION_DIM)\n",
    "        encoded_direction_dim = direction_encoder.get_last_dim(DIRECTION_DIM)\n",
    "\n",
    "        self.position_hidden_layer_skip_indexs = set(\n",
    "            [i for i in range(1, layer_count - 1) if i % 4 == 0]\n",
    "        )\n",
    "        self.position_input_layer = nn.Linear(\n",
    "            encoded_position_dim,\n",
    "            hidden_dim,\n",
    "            device=device,\n",
    "        )\n",
    "        self.position_hidden_layers = nn.ModuleList(\n",
    "            [\n",
    "                (\n",
    "                    nn.Linear(\n",
    "                        hidden_dim + encoded_position_dim,\n",
    "                        hidden_dim,\n",
    "                        device=device,\n",
    "                    )\n",
    "                    if i in self.position_hidden_layer_skip_indexs\n",
    "                    else nn.Linear(\n",
    "                        hidden_dim,\n",
    "                        hidden_dim,\n",
    "                        device=device,\n",
    "                    )\n",
    "                )\n",
    "                for i in range(layer_count)\n",
    "            ]\n",
    "        )\n",
    "        self.density_output_layer = nn.Linear(\n",
    "            hidden_dim,\n",
    "            DENSITY_DIM,\n",
    "            device=device,\n",
    "        )\n",
    "        self.direction_input_layer = nn.Linear(\n",
    "            hidden_dim + encoded_direction_dim,\n",
    "            additional_hidden_dim,\n",
    "            device=device,\n",
    "        )\n",
    "        self.color_output_layer = nn.Linear(\n",
    "            additional_hidden_dim, COLOR_DIM, device=device\n",
    "        )\n",
    "\n",
    "        self.position_input_encoder = position_encoder\n",
    "        self.direction_input_encoder = direction_encoder\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        import torch\n",
    "\n",
    "        inputs = torch.as_tensor(inputs)\n",
    "\n",
    "        positions = inputs[..., 0:3]\n",
    "        directions = inputs[..., 3:6]\n",
    "        encoded_positions: Tensor = self.position_input_encoder(positions)\n",
    "        encoded_directions: Tensor = self.direction_input_encoder(directions)\n",
    "\n",
    "        hidden_positions: Tensor = self.position_input_layer(encoded_positions).relu()\n",
    "        for index, layer in enumerate(self.position_hidden_layers):\n",
    "            hidden_positions = layer(\n",
    "                torch.concat([hidden_positions, encoded_positions], dim=-1)\n",
    "                if index in self.position_hidden_layer_skip_indexs\n",
    "                else hidden_positions\n",
    "            )\n",
    "            hidden_positions = hidden_positions.relu()\n",
    "\n",
    "        density: Tensor = self.density_output_layer(hidden_positions).relu()\n",
    "        hidden_directions: Tensor = self.direction_input_layer(\n",
    "            torch.concat([hidden_positions, encoded_directions], dim=-1)\n",
    "        ).relu()\n",
    "        color: Tensor = self.color_output_layer(hidden_directions).sigmoid()\n",
    "\n",
    "        return torch.concat([color, density], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Weight Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.optimizer import ParamsT\n",
    "\n",
    "\n",
    "class AdamWeightUpdater(Adam):\n",
    "    def __init__(self, parameters: ParamsT, learning_rate: float | None = None):\n",
    "        learning_rate = float(learning_rate or 5e-4)\n",
    "\n",
    "        super(AdamWeightUpdater, self).__init__(parameters, lr=learning_rate)\n",
    "\n",
    "    def __call__(self, loss: Tensor) -> None:\n",
    "        self.zero_grad()\n",
    "        loss.backward()\n",
    "        self.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Ray Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.types import Device\n",
    "\n",
    "\n",
    "class RayGenerator(Module):\n",
    "    \"\"\"\n",
    "    ## Arguments\n",
    "    - `height`: `int`\n",
    "    - `width`: `int`\n",
    "    - `focal`: `float`\n",
    "\n",
    "    ## Inputs\n",
    "    1. `posture`: `[4, 4]`\n",
    "\n",
    "    ## Outputs\n",
    "    1. `origins`: `[height, width, 3]`\n",
    "    2. `directions`: `[height, width, 3]`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        height: int,\n",
    "        width: int,\n",
    "        focal: float,\n",
    "        device: Device | None = None,\n",
    "    ):\n",
    "        super(RayGenerator, self).__init__()\n",
    "\n",
    "        import torch\n",
    "\n",
    "        focal_inverse = 1.0 / float(focal)\n",
    "        unit_half_norm = focal_inverse / 2\n",
    "        height_half_norm = height * unit_half_norm\n",
    "        width_half_norm = width * unit_half_norm\n",
    "\n",
    "        self.directions = torch.stack(\n",
    "            torch.meshgrid(\n",
    "                torch.arange(\n",
    "                    -width_half_norm + unit_half_norm,\n",
    "                    width_half_norm,\n",
    "                    focal_inverse,\n",
    "                    device=device,\n",
    "                ),\n",
    "                torch.arange(\n",
    "                    height_half_norm - unit_half_norm,\n",
    "                    -height_half_norm,\n",
    "                    -focal_inverse,\n",
    "                    device=device,\n",
    "                ),\n",
    "                torch.tensor(-1.0, device=device),\n",
    "                indexing=\"xy\",\n",
    "            ),\n",
    "            dim=-1,\n",
    "        ).unsqueeze_(-2)\n",
    "\n",
    "    def forward(self, posture: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        posture = torch.as_tensor(posture)[:3]\n",
    "\n",
    "        directions = (self.directions * posture[:, :3]).sum(dim=-1)\n",
    "        origins = posture[:, 3].broadcast_to(directions.shape)\n",
    "        return origins, directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Stratified Point Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "class StratifiedPointSampler(Module):\n",
    "    \"\"\"\n",
    "    ## Arguments\n",
    "    - `points_per_ray`: `int`\n",
    "\n",
    "    ## Inputs\n",
    "    1. `origins`: `[..., 1, 3]`\n",
    "    2. `directions`: `[..., 1, 3]`\n",
    "\n",
    "    ## Outputs\n",
    "    1. `points`: `[..., points_per_ray, 3]`\n",
    "    2. `intervals`: `[..., points_per_ray]` (Ended with `1e9`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, points_per_ray: int | None = None, seed: int | None = None):\n",
    "        super(StratifiedPointSampler, self).__init__()\n",
    "\n",
    "        from torch import Generator\n",
    "\n",
    "        points_per_ray = int(points_per_ray or 32)\n",
    "        if seed is not None:\n",
    "            seed = int(seed)\n",
    "\n",
    "        self.generator: Generator = seed\n",
    "        self.points_per_ray = points_per_ray\n",
    "\n",
    "    def forward(self, origins: Tensor, directions: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        origins = torch.as_tensor(origins)\n",
    "        directions = torch.as_tensor(directions)\n",
    "\n",
    "        device = origins.device\n",
    "        if type(self.generator) is int:\n",
    "            self.generator = torch.Generator(device).manual_seed(self.generator)\n",
    "\n",
    "        distances = (\n",
    "            torch.linspace(2.0, 6.0, self.points_per_ray, device=device)\n",
    "            .repeat(*origins.shape[:-2], 1)\n",
    "            .add_(\n",
    "                torch.rand(\n",
    "                    *origins.shape[:-2],\n",
    "                    self.points_per_ray,\n",
    "                    device=device,\n",
    "                    generator=self.generator,\n",
    "                )\n",
    "            )\n",
    "            .mul_(4.0 / self.points_per_ray)\n",
    "        )\n",
    "        intervals = torch.concat(\n",
    "            [\n",
    "                distances[..., 1:] - distances[..., :-1],\n",
    "                torch.tensor(1, device=device).repeat(\n",
    "                    (*origins.shape[:-2], 1),\n",
    "                ),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        points = origins + directions * distances.unsqueeze_(-1)\n",
    "        return points, intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Volume Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "class VolumeRenderer(Module):\n",
    "    \"\"\"\n",
    "    ## Inputs\n",
    "    1. `colors` + `densities`: `[..., points_per_ray, 3 + 1]`\n",
    "    2. `intervals`: `[..., points_per_ray]`\n",
    "\n",
    "    ## Outputs\n",
    "    1. `rendered_colors`: `[..., 3]`\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, inputs: Tensor, intervals: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        inputs = torch.as_tensor(inputs)\n",
    "\n",
    "        colors = inputs[..., :3]\n",
    "        densities = inputs[..., 3]\n",
    "        translucency = (-densities * intervals).exp()\n",
    "        transmittance = (1.0 - translucency) * torch.cumprod(translucency, dim=-1)\n",
    "        rendered_colors = (transmittance.unsqueeze(-1) * colors).sum(dim=-2)\n",
    "        return rendered_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class MSELossFunction:\n",
    "    def __call__(\n",
    "        self,\n",
    "        rendered_color: Tensor,\n",
    "        target_color: Tensor,\n",
    "    ) -> Tensor:\n",
    "        from torch import as_tensor\n",
    "\n",
    "        rendered_color = as_tensor(rendered_color)\n",
    "        target_color = as_tensor(target_color)\n",
    "\n",
    "        loss = (rendered_color - target_color).square().mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "class DataLoader(Module):\n",
    "    def __init__(self, batch_size: int | None = None):\n",
    "        super(DataLoader, self).__init__()\n",
    "\n",
    "        batch_size = int(batch_size or 1)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Iterator[Tensor]:\n",
    "        from torch.utils.data import DataLoader as _DataLoader\n",
    "\n",
    "        return iter(_DataLoader(inputs, batch_size=self.batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch import Tensor\n",
    "from torch.types import Device\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ViewSynthesisDataset:\n",
    "    count: int\n",
    "    focal: float\n",
    "    height: int\n",
    "    images: Tensor\n",
    "    postures: Tensor\n",
    "    width: int\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if self.images.shape[0] != self.postures.shape[0]:\n",
    "            raise ValueError(\"The number of images and postures must be the same\")\n",
    "\n",
    "    @staticmethod\n",
    "    def from_numpy(url: str) -> \"ViewSynthesisDataset\":\n",
    "        from httpx import get\n",
    "        from io import BytesIO\n",
    "        from numpy import load\n",
    "\n",
    "        import torch\n",
    "\n",
    "        try:\n",
    "            file = BytesIO(\n",
    "                get(\n",
    "                    url,\n",
    "                    follow_redirects=True,\n",
    "                    timeout=60,\n",
    "                )\n",
    "                .raise_for_status()\n",
    "                .content\n",
    "            )\n",
    "        except:\n",
    "            file = open(url, \"rb\")\n",
    "\n",
    "        with file as file_entered:\n",
    "            arrays = load(file_entered)\n",
    "            focal = float(arrays[\"focal\"])\n",
    "            images = torch.as_tensor(arrays[\"images\"])\n",
    "            postures = torch.as_tensor(arrays[\"poses\"])\n",
    "\n",
    "        return ViewSynthesisDataset(\n",
    "            count=images.shape[0],\n",
    "            focal=focal,\n",
    "            height=images.shape[1],\n",
    "            images=images,\n",
    "            postures=postures,\n",
    "            width=images.shape[2],\n",
    "        )\n",
    "\n",
    "    def get_image_and_posture(self, index: int | None = None) -> tuple[Tensor, Tensor]:\n",
    "        from random import randint\n",
    "\n",
    "        if index is not None:\n",
    "            index = int(index)\n",
    "        else:\n",
    "            index = randint(0, self.count - 1)\n",
    "\n",
    "        return self.images[index], self.postures[index]\n",
    "\n",
    "    def set_device(self, device: Device) -> \"ViewSynthesisDataset\":\n",
    "        self.images = self.images.to(device)\n",
    "        self.postures = self.postures.to(device)\n",
    "        return self\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        repr = f\"{self.__class__.__name__}(\"\n",
    "        for name, value in self.__dict__.items():\n",
    "            if isinstance(value, Tensor):\n",
    "                value = f\"Tensor(shape={tuple(value.shape)}, dtype={value.dtype})\"\n",
    "            elif type(value) is float:\n",
    "                value = f\"{value:.7f}\"\n",
    "            repr += f\"\\n  {name}={value},\"\n",
    "        repr += \"\\n)\"\n",
    "        return repr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### NeRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.types import Device\n",
    "\n",
    "\n",
    "class NeRF(Module):\n",
    "    \"\"\"\n",
    "    ## Arguments\n",
    "    - `focal`: `float`\n",
    "    - `height`: `int`\n",
    "    - `width`: `int`\n",
    "\n",
    "    ## Inputs\n",
    "    1. `posture`: `[4, 4]`\n",
    "\n",
    "    ## Outputs\n",
    "    1. `rendered_image`: `[height, width, 3]`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        focal: float,\n",
    "        height: int,\n",
    "        width: int,\n",
    "        points_per_ray: int | None = None,\n",
    "        rays_per_batch: int | None = None,\n",
    "        device: Device | None = None,\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        from random import seed as set_seed\n",
    "\n",
    "        super(NeRF, self).__init__()\n",
    "\n",
    "        points_per_ray = int(points_per_ray or 32)\n",
    "        rays_per_batch = int(rays_per_batch or 1024)\n",
    "        if seed is not None:\n",
    "            seed = int(seed)\n",
    "            set_seed(seed)\n",
    "\n",
    "        points_per_batch = points_per_ray * rays_per_batch\n",
    "\n",
    "        self.generate_rays = RayGenerator(\n",
    "            focal=focal,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            device=device,\n",
    "        )\n",
    "        self.sample = StratifiedPointSampler(\n",
    "            points_per_ray=points_per_ray,\n",
    "            seed=seed,\n",
    "        )\n",
    "        self.load_batches = DataLoader(\n",
    "            batch_size=points_per_batch,\n",
    "        )\n",
    "        self.predict = MLP(\n",
    "            layer_count=8,\n",
    "            hidden_dim=256,\n",
    "            additional_hidden_dim=128,\n",
    "            position_encoder=PositionalEncoder(10, device=device),\n",
    "            direction_encoder=PositionalEncoder(4, device=device),\n",
    "            device=device,\n",
    "        )\n",
    "        self.render = VolumeRenderer()\n",
    "\n",
    "    def forward(self, posture: Tensor) -> Tensor:\n",
    "        import torch\n",
    "\n",
    "        origins: Tensor\n",
    "        directions: Tensor\n",
    "        positions: Tensor\n",
    "        intervals: Tensor\n",
    "\n",
    "        origins, directions = self.generate_rays(posture)\n",
    "        positions, intervals = self.sample(origins, directions)\n",
    "        positions_and_directions = torch.concat(\n",
    "            [\n",
    "                positions,\n",
    "                directions.broadcast_to(positions.shape),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        ).flatten(\n",
    "            end_dim=-2,\n",
    "        )\n",
    "        colors_and_densities = torch.concat(\n",
    "            [\n",
    "                self.predict(batch)\n",
    "                for batch in self.load_batches(positions_and_directions)\n",
    "            ],\n",
    "            dim=0,\n",
    "        ).reshape(\n",
    "            (*positions.shape[:-1], 4),\n",
    "        )\n",
    "        rendered_image = self.render(colors_and_densities, intervals)\n",
    "        # display(\n",
    "        #     dict(\n",
    "        #         origins=origins[0, 0],\n",
    "        #         directions=directions[0, 0],\n",
    "        #         positions=positions[0, 0, 0],\n",
    "        #         intervals=intervals[0, 0],\n",
    "        #         colors_densities=colors_and_densities[0, 0],\n",
    "        #         rendered_image=rendered_image[0, 0],\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "        return rendered_image\n",
    "\n",
    "    @staticmethod\n",
    "    def fit(\n",
    "        dataset: ViewSynthesisDataset,\n",
    "        epochs: int | None = None,\n",
    "        learning_rate: float | None = None,\n",
    "        show_progress: bool | None = None,\n",
    "        *,\n",
    "        points_per_ray: int | None = None,\n",
    "        rays_per_batch: int | None = None,\n",
    "        device: Device | None = None,\n",
    "        seed: int | None = None,\n",
    "    ) -> \"NeRF\":\n",
    "        from PIL import Image\n",
    "        import torch\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        EPOCHS_PER_DEMO = 50\n",
    "\n",
    "        dataset.set_device(device)\n",
    "        epochs = int(epochs or 1)\n",
    "        if show_progress is None:\n",
    "            show_progress = True\n",
    "\n",
    "        model = NeRF(\n",
    "            focal=dataset.focal,\n",
    "            height=dataset.height,\n",
    "            width=dataset.width,\n",
    "            points_per_ray=points_per_ray,\n",
    "            rays_per_batch=rays_per_batch,\n",
    "            device=device,\n",
    "            seed=seed,\n",
    "        )\n",
    "        calculate_loss = MSELossFunction()\n",
    "        update_weight = AdamWeightUpdater(\n",
    "            parameters=model.parameters(),\n",
    "            learning_rate=learning_rate,\n",
    "        )\n",
    "        progress = tqdm(\n",
    "            disable=not show_progress,\n",
    "            desc=f\"Fitting NeRF model to {dataset.count}x images and postures\",\n",
    "            ascii=\"         |\",\n",
    "            colour=\"blue\",\n",
    "            dynamic_ncols=True,\n",
    "            total=epochs,\n",
    "        )\n",
    "\n",
    "        with progress:\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                image, posture = dataset.get_image_and_posture()\n",
    "                rendered_image: Tensor = model(posture)\n",
    "                loss = calculate_loss(rendered_image, image)\n",
    "                update_weight(loss)\n",
    "                progress.update()\n",
    "\n",
    "                if show_progress and epoch % EPOCHS_PER_DEMO == 0:\n",
    "                    with torch.no_grad():\n",
    "                        posture_demo = dataset.get_image_and_posture(-1)[1]\n",
    "                        rendered_image_demo = model(posture_demo)\n",
    "                        display(\n",
    "                            Image.fromarray(\n",
    "                                (rendered_image_demo * 255)\n",
    "                                .round()\n",
    "                                .type(torch.uint8)\n",
    "                                .numpy()\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = ViewSynthesisDataset.from_numpy(\"http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ViewSynthesisDataset(\n",
       "   count=85,\n",
       "   focal=138.8888789,\n",
       "   height=100,\n",
       "   images=Tensor(shape=(85, 100, 100, 3), dtype=torch.float32),\n",
       "   postures=Tensor(shape=(85, 4, 4), dtype=torch.float32),\n",
       "   width=100,\n",
       " ),\n",
       " ViewSynthesisDataset(\n",
       "   count=21,\n",
       "   focal=138.8888789,\n",
       "   height=100,\n",
       "   images=Tensor(shape=(21, 100, 100, 3), dtype=torch.float32),\n",
       "   postures=Tensor(shape=(21, 4, 4), dtype=torch.float32),\n",
       "   width=100,\n",
       " ))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_count = original_dataset.count // 5\n",
    "test_dataset = ViewSynthesisDataset(\n",
    "    count=test_data_count,\n",
    "    focal=original_dataset.focal,\n",
    "    height=original_dataset.height,\n",
    "    images=original_dataset.images[-test_data_count:],\n",
    "    postures=original_dataset.postures[-test_data_count:],\n",
    "    width=original_dataset.width,\n",
    ")\n",
    "train_dataset = ViewSynthesisDataset(\n",
    "    count=original_dataset.count - test_data_count,\n",
    "    focal=original_dataset.focal,\n",
    "    height=original_dataset.height,\n",
    "    images=original_dataset.images[:-test_data_count],\n",
    "    postures=original_dataset.postures[:-test_data_count],\n",
    "    width=original_dataset.width,\n",
    ")\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting NeRF model to 85x images and postures:   0%|\u001b[34m          \u001b[0m| 1/10000 [00:13<36:38:20, 13.19s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mNeRF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrays_per_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoints_per_ray\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 158\u001b[0m, in \u001b[0;36mNeRF.fit\u001b[0;34m(dataset, epochs, learning_rate, show_progress, points_per_ray, rays_per_batch, device, seed)\u001b[0m\n\u001b[1;32m    156\u001b[0m rendered_image: Tensor \u001b[38;5;241m=\u001b[39m model(posture)\n\u001b[1;32m    157\u001b[0m loss \u001b[38;5;241m=\u001b[39m calculate_loss(rendered_image, image)\n\u001b[0;32m--> 158\u001b[0m \u001b[43mupdate_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m progress\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show_progress \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m EPOCHS_PER_DEMO \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mAdamWeightUpdater.__call__\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/practice/nerf/.venv/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/practice/nerf/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/practice/nerf/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = NeRF.fit(\n",
    "    dataset=train_dataset,\n",
    "    device=\"cpu\",\n",
    "    epochs=10000,\n",
    "    rays_per_batch=1000,\n",
    "    points_per_ray=4,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
